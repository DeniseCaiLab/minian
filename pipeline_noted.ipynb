{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "<style> code {background-color : lightgrey !important;} </style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About this document\n",
    "\n",
    "The purpose of this noted version of pipeline is to guide the user through each step of the codes so that they are able to understand them as much as possible and equipped with knowledge to customize the script regardless of programming skills. It is **not** supposed to be runned as production tool. If you click \"Run All\" on this document, you **will** encounter errors. For productive purpose, consider modifying the accompanying **pipeline.ipynb** and **batch_processing.ipynb** to suit your need.\n",
    "\n",
    "Before we start, it's highly recommended that you get familiar with basic python concepts and operations like [string manipulation](https://docs.python.org/3.4/library/string.html), [tuples, lists and dictionaries](https://docs.python.org/3/tutorial/datastructures.html), as well as a little bit about [object-oriented programming](https://python.swaroopch.com/oop.html) and [python modules](https://docs.python.org/3/tutorial/modules.html).\n",
    "\n",
    "Another note on the styling of this document: most of the sentences should (hopefully) make sense if taken literally. However, I try to use some special formatting on the texts consistently to demonstrate the close relationship between the codes and thought process, as well as encouraging the reader to understand the already natural and self-explantory Python syntax and jargons. Specifically:\n",
    "\n",
    "-  a [hyperlink](https://en.wikipedia.org/wiki/Hyperlink) usually point to a well-defined python module or class or methods, especially when that concept is first encountered in this document and the reader would likely need some background reference. The link usually points to the official documentation of that concept, which might not be the best place to start for beginner. If you find the documentation puzzling, try to google that concept and find a tutorial that best suit your preference.\n",
    "-  an inline `code` usually refer to a name that already exsist in the [namespace](https://docs.python.org/3/tutorial/classes.html#python-scopes-and-namespaces) (i.e. the context where we run the codes in this document). It can be a previously encountered concepts, but more often it referes to variable or method names that we [imported](https://docs.python.org/3/reference/import.html) or defined along the way.\n",
    "-  **bold** texts are used more loosely to highlight anything that requires more attention than plain texts. Though it's not used as carefully as previous formats, it often refer to some specific values that a variable or method arguments can assume.\n",
    "-  <div class=\"alert alert-info\">\n",
    "    Blue tip boxes are used to provide direct instructions and coding tips to help users run through this pipeline smoothly! :) \n",
    "</div>\n",
    "- <div class=\"alert alert-success\" role=\"alert\">\n",
    "    Green tip boxes are used to let the user know what to expect from the output visualization result of parameter exploring.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">**Workflow**</font>\n",
    "\n",
    "As shown in the workflow below, there are 7 main sections in this pipeline, and results data will be saved after **Motion correction**, **Background removal**, **Initialization**, and **CNMF**. When you run through this and decide to restart/shutdown the kernal before you finish the whole pipeline, you don't have to re-run everything (besides of **Setting Up** module) the next time you want to pick this up! For example, you restart/shutdown the kernal after **Initialization**, next time you can simply rerun the **Setting Up**, and go directly to **CNMF**.\n",
    "\n",
    "Before we dive into the pipeline, I'd also like to introduce the most powerful and important aspect of this pipeline -- parameter exploring, which mainly happens before **Initialization** and **CNMF**. It is the interactive visualization step that makes all parameters transparent to the users, and help the users to find the best parameters suit for their dataset. We will get into more details once we reach there!\n",
    "\n",
    "![workflow](img/Workflow_v2.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells under this section should be executed every time the kernel is restarted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell loads the necessary modules and usually should not be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import os\n",
    "import gc\n",
    "import psutil\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import holoviews as hv\n",
    "import matplotlib.pyplot as plt\n",
    "import bokeh.plotting as bpl\n",
    "import dask.array as da\n",
    "import pandas as pd\n",
    "import dask\n",
    "import datashader as ds\n",
    "import itertools as itt\n",
    "import papermill as pm\n",
    "import ast\n",
    "import functools as fct\n",
    "from holoviews.operation.datashader import datashade, regrid, dynspread\n",
    "from datashader.colors import Sets1to3\n",
    "from dask.diagnostics import ProgressBar\n",
    "from IPython.core.display import display, HTML\n",
    "from dask.distributed import Client, progress, LocalCluster, fire_and_forget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## set path and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set all the parameters that controls how the notebook will behave. Ideally, the following cell would be the only thing you have to change when analyzing different datasets. We put all parameters here under a single cell to facilitate batch processing, but most of them will not make sense until we reach and discuss the corresponding steps. So in this subsecsion, we discuss only some initial basic parameters that has broader impact, and leave the discussion of specific parameters to each step.\n",
    "* `minian_path` should be the path that contains a folder named **\"minian\"** , under which the actual codes of **minian** (.py files) resides. The default value **\\.** means \"current folder\", which should work in most cases unless you want to try out another specific version of minian that is not in the same folder as this notebook.\n",
    "* `dpath` is the folder that contains the actual videos, which are usually named **\"msCam\\*.avi\"** where \\* is a number.\n",
    "* `chunks` is used to divide data into smaller chunks for parallel processing. Currently it is rarelly used and has little impact on the actual performance of the script. It is recommended to leave chunks to default. \n",
    "\n",
    "\n",
    "* `in_memory` specify whether to load the whole dataset into memory.\n",
    "* `interactive` controls whether interactive plots should be shown.  Not that plotting the results implies actual computation to be carried out, and thus could be very inefficient when the data are not in the memory. \n",
    "<div class=\"alert alert-info\">\n",
    "In real situation, when you want to visualize interactive plots while figuring out the best parameters, you would want in_memory=True and interactive=True as long as your data can fit in the memory. On the other hand, once you finalized your parameters and ready for batch processing, you would want both of them to be set as False.\n",
    "</div>\n",
    "\n",
    "\n",
    "* `output_size` controls the realtive size of all the plots on a scale of 0-100 percent, though it can goes beyond 100 without any problem. Adjust this to please your eye.\n",
    "\n",
    "\n",
    "\n",
    "* `param_save_minian` specify the data saving property for all the saving part. `dpath` is where you want the data to be saved, we recommand use the same dpath as where you load the imaging data. `fname` is the name of your dataset. In `backend`, `'zarr'` is designed for parallel and out-of-core computation, but its support is experimental for now and does not support incremental writing (i.e. it will be a pain to update part of exsisting dataset). `meta_dict` is a `dictionary` that is used to construct meta data for the final labeled data structure. \n",
    "<div class=\"alert alert-info\">\n",
    "The defult meta_dict is assuming data is stored in some structured folders as shown below. We do <strong> recommand </strong> users to structure their data like this so that they don't have to adjust this meta_dict setting. However, if you already have a preferred way to store youre data, you can simply change the value of meta_dict in this parameter to fit your data!\n",
    "</div>\n",
    "\n",
    "### Recommend Folder Structure\n",
    "![Folder Structure](img/folder_structure.png)\n",
    "\n",
    "The default value means: the name of the last folder (`values`=-1) in `dpath` (the one that directly contains the videos) would be used as the value of a field named **'session_id'**, the name of the second-last folder (`values`=-2) in `dpath` would be the value for **'session'** and so on. Both the `keys` (field names) and `values` (numbers indicating which level of folder name should be used) of `meta_dict` can be modified to suit the specific user case. `overwrite` is a boolean value controlling whether the data is overwritten when the file already exsists. We set it to `True` here so you can easily play with the demo multiple times, but **use extreme caution** with this during actual analysis -- it won't ask again for your confirmation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#Set up Initial Basic Parameters#\n",
    "minian_path = \".\"\n",
    "dpath = \"./demo_movies\"\n",
    "chunks = {\"frame\": 1000, \"height\": 50, \"width\": 50, \"unit_id\": 100}\n",
    "in_memory = False\n",
    "interactive = True\n",
    "output_size = 60\n",
    "param_save_minian = {\n",
    "    'dpath': dpath,\n",
    "    'fname': 'minian',\n",
    "    'backend': 'zarr',\n",
    "    'meta_dict': dict(seesion_id=-1, session=-2, animal=-3),\n",
    "    'overwrite': True}\n",
    "\n",
    "#Pre-processing Parameters#\n",
    "param_load_videos = {\n",
    "    'pattern': 'msCam[0-9]+\\.avi$',\n",
    "    'dtype': np.float32,\n",
    "    'in_memory': in_memory,\n",
    "    'downsample': dict(frame=2),\n",
    "    'downsample_strategy': 'subset'}\n",
    "subset = None\n",
    "param_glow_removal = {\n",
    "    'method': 'uniform',\n",
    "    'wnd': 51}\n",
    "param_brightspot_removal = {\n",
    "    'thres': 2}\n",
    "param_first_denoise = {\n",
    "    'method': 'median',\n",
    "    'ksize': 5}\n",
    "param_second_denoise = {\n",
    "    'method': 'gaussian',\n",
    "    'sigmaX': 0,\n",
    "    'ksize': (5, 5)}\n",
    "\n",
    "#Motion Correction Parameters#\n",
    "subset_mc = None\n",
    "param_estimate_shift = {\n",
    "    'dim': 'frame',\n",
    "    'on': 'first',\n",
    "    'pad_f': 1,\n",
    "    'pct_thres': 99.99}\n",
    "\n",
    "#Background Removal Parameters#\n",
    "param_background_removal = {\n",
    "    'method': 'tophat',\n",
    "    'wnd': 50}\n",
    "\n",
    "#Initialization Parameters#\n",
    "param_seeds_init = {\n",
    "    'wnd_size': 2000,\n",
    "    'method': 'rolling',\n",
    "    'stp_size': 1000,\n",
    "    'nchunk': 100,\n",
    "    'max_wnd': 10}\n",
    "param_gmm_refine = {\n",
    "    'q': (0.1, 99.9),\n",
    "    'n_components': 2,\n",
    "    'valid_components': 1,\n",
    "    'mean_mask': True}\n",
    "param_pnr_refine = {\n",
    "    'noise_freq': 0.06,\n",
    "    'thres': 'auto',\n",
    "    'med_wnd': None}\n",
    "param_ks_refine = {\n",
    "    'sig': 0.05}\n",
    "param_seeds_merge = {\n",
    "    'thres_dist': 5,\n",
    "    'thres_corr': 0.7,\n",
    "    'noise_freq': 'envelope'}\n",
    "param_initialize = {\n",
    "    'thres_corr': 0.8,\n",
    "    'wnd': 10}\n",
    "\n",
    "#CNMF Parameters#\n",
    "param_get_noise = {\n",
    "    'noise_range': (0.06, 0.5),\n",
    "    'noise_method': 'logmexp'}\n",
    "param_first_spatial = {\n",
    "    'dl_wnd': 5,\n",
    "    'sparse_penal': 0.1,\n",
    "    'update_background': False,\n",
    "    'post_scal': True,\n",
    "    'zero_thres': 'eps'}\n",
    "param_first_temporal = {\n",
    "    'noise_freq': 0.06,\n",
    "    'sparse_penal': 1,\n",
    "    'p': 2,\n",
    "    'add_lag': 20,\n",
    "    'use_spatial': False,\n",
    "    'chk': chunks,\n",
    "    'jac_thres': 0.1,\n",
    "    'zero_thres': 1e-8,\n",
    "    'max_iters': 200,\n",
    "    'use_smooth': True,\n",
    "    'scs_fallback': False,\n",
    "    'post_scal': True}\n",
    "param_first_merge = {\n",
    "    'thres_corr': 0.9}\n",
    "param_second_spatial = {\n",
    "    'dl_wnd': 5,\n",
    "    'sparse_penal': 0.05,\n",
    "    'update_background': False,\n",
    "    'post_scal': True,\n",
    "    'zero_thres': 'eps'}\n",
    "param_second_temporal = {\n",
    "    'noise_freq': 0.06,\n",
    "    'sparse_penal': 1,\n",
    "    'p': 2,\n",
    "    'add_lag': 20,\n",
    "    'use_spatial': False,\n",
    "    'chk': chunks,\n",
    "    'jac_thres': 0.1,\n",
    "    'zero_thres': 1e-8,\n",
    "    'max_iters': 500,\n",
    "    'use_smooth': True,\n",
    "    'scs_fallback': False,\n",
    "    'post_scal': True}\n",
    "param_second_merge = {\n",
    "    'thres_corr': 0.9}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## import minian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell loads **minian** and usually should not be modified. If you encounter an `ImportError`, check that you followed the installation instructions and that `minian_path` is pointing to the right place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "sys.path.append(minian_path)\n",
    "from minian.utilities import load_params, load_videos, scale_varr, scale_varr_da, save_variable, open_minian, save_minian, handle_crash\n",
    "from minian.preprocessing import remove_brightspot, gradient_norm, denoise, remove_background, stripe_correction\n",
    "from minian.motion_correction import estimate_shift_fft, apply_shifts, interpolate_frame, mask_shifts\n",
    "from minian.initialization import seeds_init, gmm_refine, pnr_refine, intensity_refine, ks_refine, seeds_merge, initialize\n",
    "from minian.cnmf import psd_welch, psd_fft, get_noise, update_spatial, update_temporal, unit_merge, smooth_sig\n",
    "from minian.visualization import VArrayViewer, CNMFViewer, generate_videos, visualize_seeds, visualize_gmm_fit, visualize_spatial_update, visualize_temporal_update, roi_draw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## module initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell handles some initial setting of modules and parameters. They usually should not be modified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "dpath = os.path.abspath(dpath)\n",
    "para_norm_list = ['meta_dict', 'chunks', 'subset']\n",
    "for par_key in list(globals().keys()):\n",
    "    if par_key in para_norm_list or par_key.startswith('param_'):\n",
    "        globals()[par_key] = load_params(globals()[par_key])\n",
    "if interactive:\n",
    "    hv.notebook_extension('bokeh', width=100)\n",
    "    pbar = ProgressBar()\n",
    "    pbar.register()\n",
    "else:\n",
    "    hv.notebook_extension('matplotlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following few cells during pre-processing, all functions (such as `remove_brightspot`) are evaluated lazily, which means it only creates a \"plan\" for the comuptation without doing it. Actual computations are carried out when we explicitly call the [persist](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.persist.html) method of the resulting `DataArray` (like `varr_ref.persist()`). This opens up possibility for [out-of-core computations](https://en.wikipedia.org/wiki/External_memory_algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading videos and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "varr = load_videos(dpath, **param_load_videos)\n",
    "if in_memory:\n",
    "    varr = varr.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the values of `param_load_videos`:\n",
    "\n",
    "```python\n",
    "param_load_videos = {\n",
    "    'pattern': 'msCam[0-9]+\\.avi$',\n",
    "    'dtype': np.float32,\n",
    "    'in_memory': in_memory,\n",
    "    'downsample': dict(frame=2),\n",
    "    'downsample_strategy': 'subset'}\n",
    "```\n",
    "\n",
    "The first argument of `load_videos` should be the path that contains the videos, which should be `dpath` we already defined. The argument `pattern` is optional and is the [regular expression](https://docs.python.org/3/library/re.html) used to filter files under the specified folder. The default value **'msCam[0-9]+\\.avi$'** means that a file can only be loaded if its filename contain **'msCam'** then followed by at least one digit of number then **'.avi'** as the end of the filename. This can be changed to suit the naming convention of your videos. `in_memory` specify whether the whole video should be loaded into memory. `dtype` is the underlying [data types](https://docs.scipy.org/doc/numpy-1.15.0/user/basics.types.html) of the data. Usually `float32` is good enough and should be preferred to save memory demand. The resulting \"video array\" `varr` contains three dimensions: `height`, `width`, and `frame`. If you wish to downsample the video, pass in a `dictionary` to `downsample`, whose keys should be the name of dimensions and values an integer specifying how many times that dimension should be reduced. For example, `downsample=dict('frame'=2)` will temporally downsample the video with a factor of 2. Instead, if you do not wish to downsample your data, simply pass in `downsample=None`. `downsample_strategy` will assume two values: either `'subset'`, meaning downsampling are carried out simply by subsetting the data, or `'mean'`, meaning a mean will be calculated on the window of downsampling.\n",
    "\n",
    "`param_load_videos` should be specified as python [dictionary](https://docs.python.org/3/tutorial/datastructures.html#dictionaries), whose `keys` are the dimension along which subsetting should be done, and `values` specify how subsetting should be done. \n",
    "<div class=\"alert alert-info\">     \n",
    "To here, we think this is a good chance to introduce how to manipulate the parameters while you are running thorugh this pipeline! If you want to modify the parameters, you can either go back to the initial parameter setting and change it there. Or by recalling the parameter here, then modify it by changing the values in this specific parameter dictionary. For example, if you want to change the downsampling setting for your data, you can: \n",
    "</div>                                                            \n",
    "\n",
    "**Option 1--Go back to initial parameter setting code cell, change the parameters setting there, then rerun the parameter setting cell:**\n",
    "\n",
    "\n",
    "**Example 1: Stop downsampling**\n",
    "                                                               \n",
    "```python\n",
    "param_load_videos = {\n",
    "    'pattern': 'msCam[0-9]+\\.avi$',\n",
    "    'dtype': np.float32,\n",
    "    'in_memory': in_memory,\n",
    "    'downsample': dict(frame=2),\n",
    "    'downsample_strategy': 'subset'}\n",
    "```\n",
    "\n",
    "**change this to:**\n",
    "\n",
    "```python\n",
    "param_load_videos = {\n",
    "    'pattern': 'msCam[0-9]+\\.avi$',\n",
    "    'dtype': np.float32,\n",
    "    'in_memory': in_memory,\n",
    "    'downsample': None,\n",
    "    'downsample_strategy': 'subset'}\n",
    "```\n",
    "\n",
    "**Example 2: Changing the downsampling setting from by `frame` to by `height`, also change the strategy to `mean`.**\n",
    "\n",
    "```python\n",
    "param_load_videos = {\n",
    "    'pattern': 'msCam[0-9]+\\.avi$',\n",
    "    'dtype': np.float32,\n",
    "    'in_memory': in_memory,\n",
    "    'downsample': dict(frame=2),\n",
    "    'downsample_strategy': 'subset'}\n",
    "```\n",
    "\n",
    "**change this to:**\n",
    "\n",
    "```python\n",
    "param_load_videos = {\n",
    "    'pattern': 'msCam[0-9]+\\.avi$',\n",
    "    'dtype': np.float32,\n",
    "    'in_memory': in_memory,\n",
    "    'downsample': dict(height=2),\n",
    "    'downsample_strategy': 'mean'}\n",
    "```\n",
    "                                                             \n",
    "**Option 2--Insert a code cell here by click the little + symbol on the top row of jupyter notebook, then you can change the specific 'keys' with the 'value' you want to asign to them as shown below, run this new code cell you inderted in.**\n",
    "\n",
    "**Example 1: Stop downsampling**\n",
    "```python                                                           \n",
    "param_load_videos['downsample'] = None\n",
    "``` \n",
    "\n",
    "**Example 2: Changing the downsampling setting from by `frame` to by `height`, also change the strategy to `mean`.**\n",
    "```python                                                           \n",
    "param_load_videos['downsample'] = dict(height=2)\n",
    "param_load_videos['strategy'] = 'mean'\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous code load videos and concatenate them together into `varr`, which is a [xarray.DataArray](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.html#xarray.DataArray). Now it's a perfect time to get familiar with this data structure and the [xarray](https://xarray.pydata.org/en/stable/) module in general, since we will be using these data structures throughout analysis. Basicly a `xarray.DataArray` is a labeled N-dimensional array. We can ask the computer to print out some information of `varr` by calling its name (as with any other variable):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see now that `varr` is a `xarray.DataArray` with a [name](https://xarray.pydata.org/en/stable/generated/xarray.DataArray.name.html#xarray.DataArray.name) `'demo_movies'` and three dimensions: `frame`, `height` and `width`, each dimension is simply labeled with ascending natural numbers. The [dtype](https://xarray.pydata.org/en/stable/generated/xarray.DataArray.dtype.html#xarray.DataArray.dtype) ([data type](https://docs.scipy.org/doc/numpy-1.14.0/user/basics.types.html)) of `varr` is `numpy.float32`\n",
    "\n",
    "In addition to these information, we can visualize `varr` with the help of `VArrayViewer`, which shows the array as movie and also plot max, mean and minimum fluorescence across `frame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    vaviewer = VArrayViewer(varr, framerate=5, compute=True)\n",
    "    display(vaviewer.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subset part of video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding to pre-processing, it's good practice to check if there is something obviously wrong with the video (like camera suddenly drops dark). This can usually be observed by visualizing the video and check the mean fluorescence plot. To take out some bad `frame`, let's say, `frame` after 800, we can utilize the [xarray.DataArray.sel](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.sel.html) method and [slice](https://docs.python.org/3/library/functions.html#slice):\n",
    "<div class=\"alert alert-info\">                                                               \n",
    "Here is good chance to briefly introduce how to use <strong>.sel</strong> and <strong>slice</strong> properly since this will be super useful to handle your imaging video data.  <strong>See below:</strong>\n",
    "</div>   \n",
    "\n",
    "By using DataArray.sel, you can easily use the basic format:\n",
    "```python\n",
    "subsetDataArray = DataArray.sel(dimsA=object(value))\n",
    "```\n",
    "This will assign the data array you selected out to the new data array which is `subsetDataArray` in this case.\n",
    "\n",
    "Using slice() function, you can simply subset out a portion of your data out by calling the function like:\n",
    "```python\n",
    "subsetDimsA = slice(start, stop, step)\n",
    "```\n",
    "You can combine xarray.DataArray.sel method and slice function to manipulate your multidimensional imaging data! Note that slice object is just one of the useful objects that you can use here. \n",
    "\n",
    "**Example 1**: Say that you want to get rid of the frames after 800: \n",
    "\n",
    "```python \n",
    "varr_ref = varr.sel(frame=slice(None, 800))\n",
    "```\n",
    "This will subset `varr` along the `frame` dimension from the begining to the `frame` labeled **800**, then assign the result back to `varr_ref`, which is equivalent to taking out `frame` from **801** to the end. Note you can do the same thing to other dimensions like `height` and `width` to take out certain pixels of your video for all the `frame`s. For more information on using `xarray.DataArray.sel` as well as other indexing strategies, see [xarray documentation](http://xarray.pydata.org/en/stable/indexing.html)\n",
    "\n",
    "**Example 2**: When you want to get rid of the timestamp which locates at the last row of pixels in height dimention for all the wireless miniscope recording, **.isel** will be a useful tool here! See [xarray.Dataset.isel documentation](http://xarray.pydata.org/en/stable/generated/xarray.Dataset.isel.html) Different from **.sel**, **.isel** subset out data by index: \n",
    "```python\n",
    "varr_ref = varr.isel(height=slice(None, -1))\n",
    "```\n",
    "This will subset `varr` along the `height` dimension from the begining to second to last row of pixels, then assign the result back to `varr_ref`, which is equivalent to taking out last row of `height`. \n",
    "\n",
    "If your `varr` is fine, just assign it to `varr_ref` to keep the naming consitent with later codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varr_ref = varr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In production mode** -- pipeline.ipynb or batch_processing.ipynb, we usually use the `subset` parameter defined above to control this. It a good chance for us to recall the `subset` parameter. \n",
    "```python\n",
    "subset = None\n",
    "```\n",
    " `subset` is used to subset the data, which operates on the whole data. `subset` should be specified as python [dictionary](https://docs.python.org/3/tutorial/datastructures.html#dictionaries), whose `keys` are the dimension along which subsetting should be done, and `values` specify how subsetting should be done (usually a [slice object](https://docs.python.org/3/c-api/slice.html)). A good usecase for this is to take out some troublesome frames or take out some bad pixels, such as those used as timestamps in wireless miniscope recording.\n",
    "<div class=\"alert alert-info\">                                                               \n",
    "Here is good chance to introduce how to create a dictionary, and more importantly, how to use it! <strong>See below:</strong>\n",
    "</div>\n",
    "\n",
    "\n",
    "The basic format of a dictionary will be like\n",
    "```python\n",
    "dictionary = {'key1': value1, 'key2': value2, ... 'keyN': valueN}\n",
    "```\n",
    "                                                           \n",
    "For example, if you only want to keep the first 800 `frames`, and the `height` and `width` from 100 to 200, you can create the subset dictionary like this:\n",
    "\n",
    "```python\n",
    "subset = {\n",
    "    'frame': slice(0, 800),\n",
    "    'height': slice(100, 200),\n",
    "    'width': slice(100, 200)}\n",
    "```\n",
    "                                                               \n",
    "Similarly, after you have a dictionary, you can call a specific key in this dictionary and change the value accordingly. We can use the same example here, say if you want to get rid of `frames` from 801 to the end:\n",
    "\n",
    "```python\n",
    "subset['frame'] = slice(None, 800)\n",
    "```\n",
    "<div class=\"alert alert-info\">                                                               \n",
    "Leaving subset in defult setting will result in no selection and thus equivalent to assigning varr back to varr_ref.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varr_ref = varr.sel(subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we drop along `frame` dimension `where` the frame is absolutely dark (the `sum` of all pixel values are zero). Note this step cannot be lazy, i.e. the actual `sum` computation has to be carried out here, so it might not be very efficient if `in_memory=False`. Feel free to skip this step if you are sure there is no such black frames in your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varr_ref = varr_ref.where(varr_ref.sum(['height', 'width']) > 0).dropna('frame')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we `chunk` the data into pieces to have better performance. Usually this should not be changed unless you have good understanding of [chunking and performance](http://docs.dask.org/en/latest/array-chunks.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varr_ref = varr_ref.chunk(dict(frame=int(chunks['frame']/10), height=-1, width=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stripe correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The earlier versions of miniscopes suffer from artifacts that looks like horizontal or vertical stripes across the image that's stationary across frames. `stripe_correction` tries to correct for this type of artifacts. Technically it calculate the mean along `'frame'` dimension (a \"mean frame\" if you will), then again calculate a mean along `reduce_dim` to get a one-dimensional representation of the \"stripes\", and then every frame in the movie is subtracted by this 1d stripe uniformly. Thus, `reduce_dim` should be the dimension (direction) the stripes run along. If you believe your videos are free from the stripes artifacts, feel free to skip this step to reduce artificial assumptions and biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#varr_ref = stripe_correction(varr_ref, reduce_dim='width')\n",
    "#if in_memory:\n",
    "#    with ProgressBar():\n",
    "#        varr_ref = varr_ref.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## glow removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varr_ref = remove_background(varr_ref, **param_glow_removal)\n",
    "if in_memory:\n",
    "    varr_ref = varr_ref.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use the general `remove_background` function to remove the glow caused by vignetting frame by frame. Recall tha values of `param_glow_removal`:\n",
    "\n",
    "```python\n",
    "param_glow_removal = {\n",
    "    'method': 'uniform',\n",
    "    'wnd': 51}\n",
    "```\n",
    "\n",
    "The glow/background is estimated as the `'uniform'` filtered version of the original image with a very large window size `wnd=51`, and then the background is subtracted from the original image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bright spots removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`remove_brightspot` tries to correct for brightspots that are probably caused by dust on the imaging sensor. Usually this type of artifacts occupies one pixel which is significantly brighter than its surrounding pixels. Thus, for any given pixel, `remove_brightspot` calculate a difference of intensity between that pixel and the mean of the **four** surrounding pixels (that is, four pixels that are connected to the pixel in question by edge). Then for each frame, this difference value is taken for all pixels, and a zscore of them is calculated. The pixels whose zscore of difference that exceed `thres` is treated as \"brightspots\", and whose value will be substituted by the mean of its four surrounding pixels. Thus `thres` controls how strict brightspot detection would be. In stead of passing in numbers, you can also pass a string `'min'`, which means use the inverse of the minimum (most negative) value of zscore as the threshold.\n",
    "<div class=\"alert alert-info\">                                                               \n",
    "Generally, the defult value of `param_brightspot_removal` should be good. If you believe your videos are free of this type of artifacts, feel free to skip this step to reduce artificial assumptions and biases.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "varr_ref = remove_brightspot(varr_ref, **param_brightspot_removal)\n",
    "if in_memory:\n",
    "    varr_ref = varr_ref.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## denoise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step carry out denoising of the image frame by frame. The method `denoise` takes in two required arguments: the first is the video array `varr`, and the second `method` is a string specifying the denoising method we are gonna use. Right now three methods are supported: `'gaussian'`, `'median'` and `'anisotropic'`. Under the hood, `denoise` simply call another function frame by frame in a parallel fashion. For `method='gaussian'` it calls [GaussianBlur](https://www.docs.opencv.org/3.3.0/d4/d86/group__imgproc__filter.html#gaabe8c836e97159a9193fb0b11ac52cf1) from `OpenCV` package. For`method='median'` it calls [MedianBlur](https://docs.opencv.org/3.4.3/d4/d86/group__imgproc__filter.html#ga564869aa33e58769b4469101aac458f9) from `OpenCV` package. For `method='anisotropic'` it calls [anisotropic_diffusion](http://loli.github.io/medpy/generated/medpy.filter.smoothing.anisotropic_diffusion.html) from `medpy` package. All additional [keyword arguments](https://docs.python.org/3.7/tutorial/controlflow.html#keyword-arguments) passed into `denoise` are directly passed into one of those two denoising functions under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "varr_ref = denoise(varr_ref, **param_first_denoise)\n",
    "if in_memory:\n",
    "    varr_ref = varr_ref.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we use a median filter for first pass of denoising:\n",
    "\n",
    "```python\n",
    "param_first_denoise = {\n",
    "    'method': 'median',\n",
    "    'ksize': 5}\n",
    "```\n",
    "\n",
    "There is only one parameter controlling how the filtering is done: the kernel size `ksize` of the filter.\n",
    "<div class=\"alert alert-info\">                                                               \n",
    "Generally ksize=5 is good enough. Note that if you do want to play with the ksize, it has to be odd number.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "varr_ref = denoise(varr_ref, **param_second_denoise)\n",
    "if in_memory:\n",
    "    varr_ref = varr_ref.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall for second denoising we use gaussian filter:\n",
    "\n",
    "```python\n",
    "param_second_denoise = {\n",
    "    'method': 'gaussian',\n",
    "    'sigmaX': 0,\n",
    "    'ksize': (5, 5)}\n",
    "```\n",
    "\n",
    "[Gaussian blur](https://en.wikipedia.org/wiki/Gaussian_blur) is straight-forward. All you need to do is pass in either the size of the gaussian kernel `ksize`, or the standard deviation of the kernel `sigmaX` and `sigmaY`. \n",
    "<div class=\"alert alert-info\">                                                               \n",
    "Pragmatically, ksize=(5, 5) with sigmaX=0 (meaning adjust the standard deviation according to kernel size) works fairly well.  Note that if you do want to play with the ksize, it has to be odd numbers.\n",
    "</div>\n",
    "\n",
    " For full controll of the process, refer to the [documentation](https://www.docs.opencv.org/3.3.0/d4/d86/group__imgproc__filter.html#gaabe8c836e97159a9193fb0b11ac52cf1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you can try using [anisotropic diffusion](https://en.wikipedia.org/wiki/Anisotropic_diffusion). This is inspired by [MIN1PIPE](https://github.com/JinghaoLu/MIN1PIPE) package. Refer to [the paper](https://www.cell.com/cell-reports/fulltext/S2211-1247(18)30826-X) for full detail. In a nutshell, gaussian blur can be think of as a diffusion process - that is, imagine your grey-scale image represent the concentration of some molecule, say saline, in a two dimesional dish, letting the saline diffuse for some time would be equivalent to carrying out a gaussian blur on the original image/concentration, and longer diffusion time correspond to gaussian blur with larger standard deviation. Anisotropic diffusion is a generalization of this idea - instead of diffusing uniformly, it make the diffusion slower where the gradient is higher. The result is a diffusion/denoising process that preserve the edges, and in theory is more favorable than gaussian blur. That said, `niter` is the number of iteration of this process, in other words, the \"time-step\" the diffusion is gonna last. The larger this number, the more blurred the image is gonna be. Pragmatically `niter=10` is good enought. Note that increasing this number will significantly increase the computation time since there more iterative steps for every frame. `kappa` decides who gets treated as \"edges\" and thus preserved. Pixels whose gradient is much larger than this number will diffuse much slower, whereas pixels with gradient lower than `kappa` will just flow. As mentioned before, pragmatically we found estimating `kappa` with the 90-th percentile of the gradient of an example frame works well. `gamma` control the speed of diffusion. In theory we want this as large as possible to save some `niter` and thus computation time. However it is advised to set this no larger than **0.25** otherwise weird things might happen. There were two equations that were proposed to control the diffusion process in the [original paper](https://dl.acm.org/citation.cfm?id=78304) of anisotropic diffusion, and `option` controls which one to use. Pragmatically `option=2` works better. For full control of the process, refer to the [documentation](http://loli.github.io/medpy/generated/medpy.filter.smoothing.anisotropic_diffusion.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# motion correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## select motion correction roi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we use an interactive `roi_draw` to help setting `subset_mc`. Just use the **BoxEdit** tool in the plot to draw an roi. [Here](http://holoviews.org/reference/streams/bokeh/BoxEdit.html) is the documentation on how to use that tool. Note that you need to run in interactive mode (`interactive=True`) to visualize and draw an region of interest (roi).\n",
    "<div class=\"alert alert-info\">  \n",
    "    \n",
    "<strong>Add box</strong>\n",
    "\n",
    "Hold shift then click and drag anywhere on the plot.\n",
    "\n",
    "<strong>Move box</strong>\n",
    "\n",
    "Click and drag an existing box, the box will be dropped once you let go of the mouse button.\n",
    "\n",
    "<strong>Delete box</strong>\n",
    "\n",
    "Tap a box to select it then press BACKSPACE key while the mouse is within the plot area.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    vaviewer = VArrayViewer(varr, framerate=5, compute=True)\n",
    "    display(vaviewer.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After you are satisfied with the roi, run the following cell to set `subset_mc` based on the drawing to subset the data you want the motion correction step to be based on. It's a good time to talk about `subset_mc`. It operates only on the input data for motion correction, and thus **only affect how the shifts are calculated**. A good usecase for this is if you have good landmark (e.g blood vessels or cells that are constantly on) in the field of view and want to `estimate_shifts` only on this roi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    try:\n",
    "        subset_mc = {\n",
    "            'height': slice(str_box.data['y0'][0], str_box.data['y1'][0]),\n",
    "            'width': slice(str_box.data['x0'][0], str_box.data['x1'][0])}\n",
    "    except IndexError:\n",
    "        subset_mc = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">  \n",
    "Note that if you are <strong>not</strong> running in interactive mode (interactive = False), you can also simply specify subset_mc as python <a href=\"https://docs.python.org/3/tutorial/datastructures.html#dictionaries\" class=\"alert-link\">dictionary</a></div>  \n",
    "</div>\n",
    "\n",
    "For example, you want your motion correction roi to be: height-wise from 100 to 200; width-wise from 100 to 200, you can create a dictionary like this:\n",
    "```python\n",
    "subset_mc = {'height': slice(100, 200), 'width': slice(100, 200)}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## estimate shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the parameters for `estimate shifts`:\n",
    "\n",
    "```python\n",
    "param_estimate_shift = {\n",
    "    'dim': 'frame',\n",
    "    'on': 'first',\n",
    "    'pad_f': 1,\n",
    "    'pct_thres': 99.99}\n",
    "```\n",
    "\n",
    "The idea behind `estimate_shift_fft` is simple: for each frame it calculates a two-dimensional [cross-correlation](https://en.wikipedia.org/wiki/Cross-correlation) between that frame and a template using [fft](https://en.wikipedia.org/wiki/Fast_Fourier_transform). The argument `'dim'` specify along which dimension to run the shift estimation, and should always be left `'frame'` for this pipeline. The argument `on` determine what we should use as template, it can be the first frame `on='first'`, the last frame `on='last'`, the mean frame `on='mean'`, or `on='perframe'`, meaning every frame will be registered to the immediate previous frame instead of a common template. After we have obtained the [cross-correlogram](https://en.wikipedia.org/wiki/Correlogram) for each frame, in theory we can simply look at the place/shift that has maximum correlation and use the inverse of that as `shifts`. However, due to some mysterious property of miniscope data, the maximum of the correlogram are usually biased towards weird places, most likely somewhere close to origin ((0, 0) shifts), and it's more reliable to use the [center_of_mass](https://docs.scipy.org/doc/scipy-0.16.1/reference/generated/scipy.ndimage.measurements.center_of_mass.html) of the correlogram rather than simply [argmax](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.argmax.html). There is yet another gotcha -- there are usually other local maximas that will bias `center_of_mass` and it's better to threshold those out before we take the `center_of_mass`. The argument `pct_thres` controls that -- it will set any value below that percentile to 0 in the cross-correlogram. Pragmatically `pct_thres=99.9` works well at taking out all the weird artifacts. The results from `estimate_shift_fft` are saved in a two dimensional `DataArray` called `res`, with three different `variable`s: the shifts along `'height'`, the shifts along `'width'`, and the actuall value of maximum correlation `corr`. And we have these three `variable`s across all `frame`s (hence the two dimensions). We can thus pull out `shifts` and `corr`s from `res`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "res = estimate_shift_fft(varr_ref.sel(subset_mc), **param_estimate_shift)\n",
    "if in_memory:\n",
    "    res = res.compute()\n",
    "shifts = res.sel(variable = ['height', 'width'])\n",
    "corr = res.sel(variable='corr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## masking and interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes there are frames in the video that has a horrible shift and is simply way off the common field-of-view. For these frames it is not a good idea to shift them based on cross-correlation. Instead they should be treated as data loss and discarded/interpolated. `mask_shifts` tries to address this by simply look at the maximum correlations that is saved during `estimate_shift_fft`. The idea is if some frame has a way lower cross-correlation to template comparing to other frames, they are likely to be very bad frames. `mask_shifts` takes four required arguments: `varr_fft` which is the fft-transformed video arrays. `corr` which are the values of maximum correlations. `shifts` which are the shifts in both directions. All three of these comes from `estimate_shift_fft`. The fourth one `z_thres` gets to decide who is the \"bad\" frame -- the `corr`s will be z-scored, then any frame whose z-transformed `corr` is lower than `z_thres` would be treated as \"bad\" frames and interpolated. Another optional argument is `perframe`, it's a boolean value specifying whether `estimate_shift_fft` was carried out per-frame. If `perframe=True`, in addition to taking out \"bad\" frames, it will also recalculate the shifts of the two most adjacent \"good\" frames based on each other, that is, ignoring the \"bad\" frames who just got taken out. Obviously only set `perframe=True` if `on='perframe'` for `estimate_shift_fft`. The result of `mask_shifts` is an updated version of shifts `shifts_ma`, and a boolean `mask` specifying which are the \"bad\" frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# shifts, mask = mask_shifts(varr_ref, corr, shifts, z_thres=-1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After `mask_shifts`, we use the returned `mask` to actually interpolate the bad frames. The interpolation are simply a mean frame across two most immediate adjacent \"good frames\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# varr_ref = interpolate_frame(varr_ref.compute().rename('varr_mc'), mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## determine shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### take cumulative sum if `on='perframe'` when estimating shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use `on='perframe'` for `estimate_shift_fft`, the shifts was calculated for each frame relative to the previous frame. Thus the real shift relative to the first frame should be a cumulative sum of `shifts`. \n",
    "<div class=\"alert alert-info\">  \n",
    "Run the following cell only if you on='perframe' for estimate_shift_fft\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# shifts = shifts.cumsum('frame')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualization of shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualize both the `shifts` and `shifts_final` as fluctuating curve along `frame`s. This is the first time we explicitly use the package [holoviews](http://holoviews.org) which is a really nice package for visualizing data, and I highly recommend reading through their tutorial to get familiar with the syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Curve [width=500, tools=['hover']]\n",
    "hv.output(size=output_size)\n",
    "if interactive and in_memory:\n",
    "    display(hv.NdOverlay(dict(width=hv.Curve(shifts.sel(variable='width')),\n",
    "                      height=hv.Curve(shifts.sel(variable='height')))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply shifts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we apply the `shifts_final` we got to the movie we want. After the shift all pixels that shifted inside the field of view will be filled with NaN value (`np.nan`), and we have to decide what to do with those. Here the default is to fill them with the nearest available value along `height` and `width` dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "varr_mc = apply_shifts(varr_ref, shifts)\n",
    "varr_mc = varr_mc.ffill('height').bfill('height').ffill('width').bfill('width')\n",
    "if in_memory:\n",
    "    varr_mc = varr_mc.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively you can leverage the [dropna](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.dropna.html) function to drop them, or [fillna](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.fillna.html) to fill them with a specific value (potentially `varr_mc.min()`)\n",
    "\n",
    "For example, instead of filling the shifted in pixels with the nearest available value, say you want to drop these pixels:\n",
    "```python\n",
    "varr_mc = varr_mc.where(varr_mc.isnull().sum('frame') == 0).dropna('height', how='all').dropna('width', how='all')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualization of motion-correction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again we visualize the movies with `VArrayViewer`. The optional argument `framerate` only control how the frame slider behave, not how the data is handled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive and in_memory:\n",
    "    vaviewer = VArrayViewer(varr_mc.rename('varr_mc'), framerate=5)\n",
    "    display(vaviewer.widgets)\n",
    "    display(vaviewer.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the parameters for `save_minian`:\n",
    "\n",
    "```python\n",
    "param_save_minian = {\n",
    "    'dpath': dpath,\n",
    "    'fname': 'minian',\n",
    "    'backend': 'zarr',\n",
    "    'meta_dict': dict(session_id=-1, session=-2, animal=-3),\n",
    "    'overwrite': True}\n",
    "```\n",
    "\n",
    "As we mentioned before at the **Setting up** step, the `save_minian` function decides how your data will be saved: `dpath` is the path under which the actual data file will be stored, and `fname` is the file name of the data file. `backend` can be either `'netcdf'` or `'zarr'` -- currently `'netcdf'` is more stable and is the recommended storage option from `xarray`, but it might suffer performance issue when running out-of-core computation. `'zarr'` is designed for parallel and out-of-core computation, but its support is experimental for now and does not support incremental writing (i.e. it will be a pain to update part of exsisting dataset). `meta_dict` is a `dictionary` that is used to construct meta data for the final labeled data structure which can be modified to suit the specific user's data storing structure. `overwrite` is a boolean value controlling whether the data is overwritten when the file already exsists. We set it to `True` here so you can easily play with the demo multiple times, but **use extreme caution** with this during actual analysis -- it won't ask again for your confirmation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "save_minian(shifts.rename('shifts'), **param_save_minian)\n",
    "save_minian(varr_mc.rename('org'), **param_save_minian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In particular, here we are saving our minimally-processed video `varr_mc` in `DataArray` format. We give it a \"name\" `\"org\"` by calling the [rename](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.rename.html) method on the array, which is `xarray`'s internal naming system that stays with the actual data and will be displayed when you print out the `DataArray` (In contrast, you can bind arbitrary variable name to the data like `whatever = varr_ref` without touching the data, which is less reliable and not accessible to other functions, which is probably why we need another name). In practice, give it a name that's human-readable and be sure to not name two pieces of data with the same name (Otherwise an error will occur if you try to combine them in a single dataset)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# background removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load in from disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load in the data we just saved. We use `'fname'` and `'backend'` from `param_save_minian` since they should be the same and you don't have to specify the same information twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varr_mc = open_minian(dpath,\n",
    "                      fname=param_save_minian['fname'],\n",
    "                      backend=param_save_minian['backend'])['org']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## background removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the parameters for background removal:\n",
    "\n",
    "```python\n",
    "param_background_removal = {\n",
    "    'method': 'tophat',\n",
    "    'wnd': 10}\n",
    "```\n",
    "\n",
    "This step try to estimate backgrounds (everything besides of real in-focus cells' fluorescence signal) frame by frame and remove them. As with last step, the first argument to `remove_backgroun` is `varr` which is the video array to be processed, and the second is the `method` to use. Again there are two methods available: `'uniform'` or `'tophat'`. For both of them, they require a single parameter - a window size `wnd`, which is the third required argument to `remove_background`. The two methods differ in how background is estimated. \n",
    "\n",
    "For `method='tophat'`, a [disk element](http://scikit-image.org/docs/dev/api/skimage.morphology.html#disk) with a radius of `wnd` is created, then a [morphological erosion](https://homepages.inf.ed.ac.uk/rbf/HIPR2/erode.htm) using the disk element is applied to each frame, which eats away any bright \"features\" that is smaller than the disk element. Then, a [morphological dilation](https://homepages.inf.ed.ac.uk/rbf/HIPR2/dilate.htm) is applied to the \"eroded\" image, which in theory undo the erosion except the bright \"features\" that are completely eaten away will not be recovered. The overall effect of this process is to remove any bright feature that is smaller than a disk with radius `wnd`. Thus, when setting `wnd` to the expected size of **largest** cell, this process can give us a good estimation of the background. Pragmatically **10** works pretty well.\n",
    "\n",
    "For `method='uniform'`, a [uniform filter](https://docs.scipy.org/doc/scipy-0.19.0/reference/generated/scipy.ndimage.uniform_filter.html) (basically a two dimensional rolling mean) is applied to each frame. `wnd` control the window size of the filter, and the result is used as the background. The idea is that backgrounds mainly contains out-of-focus fluorescence, that is basicly a super-blurred version of the image itself. Thus a large (comparing to the expected size of a cell) number of `wnd` should work well. Pragmatically **50** works pretty good. \n",
    "\n",
    "<div class=\"alert alert-info\"> \n",
    "The computation time of `method='tophat'` is significantly longer than `method='uniform'`, but the result is also visually much more appealing. Lastly, regardless of which `method` is chosen, `remove_background` will subtract the estimated background from the original movie frame by frame.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Y = remove_background(varr_mc, **param_background_removal)\n",
    "if in_memory:\n",
    "    with dask.config.set(scheduler='processes'):\n",
    "        Y = Y.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scale_varr` is a convenient function that act on `DataArray`s to linearly rescale every value to the range of 0 to 1 inclusive. If you don't like it, pass in `scale` to change the range or skip it altogether. Note that `scale` takes a `tuple` of 2 elements representing lower and upper bound of the range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Y = scale_varr(Y)\n",
    "if in_memory:\n",
    "    Y = Y.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualization of background removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualize the result of pre-processing. `VArrayViewer` can be used to visualize videos side by side when passed in a [list](https://docs.python.org/3/tutorial/introduction.html#lists) of video arrays. `interactive` and `in_memory` both need to be set as `True` to visualize background removal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive and in_memory:\n",
    "    vaviewer = VArrayViewer(Y.rename('Y'), framerate=5)\n",
    "    display(vaviewer.widgets)\n",
    "    display(vaviewer.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with dask.config.set(scheduler='processes'):\n",
    "    save_minian(Y.rename('Y'), **param_save_minian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole initialization section is adapted from the [MIN1PIPE](https://github.com/JinghaoLu/MIN1PIPE) package. See their [paper](https://www.cell.com/cell-reports/fulltext/S2211-1247(18)30826-X) for full details about the theories. Here we only give minimal amount of information so that we can go through the parameters.\n",
    "\n",
    "The first thing we want to do is open up the dataset we just saved.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "minian = open_minian(dpath,\n",
    "                     fname=param_save_minian['fname'],\n",
    "                     backend=param_save_minian['backend'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get the movie `Y` from the dataset, calculate a max projection that's gonna be used later, and generate a `Y_flt` where the dimemsion `'height'` and `'width'` are flattened as one dimension `spatial`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = minian['Y']\n",
    "max_proj = Y.max('frame').compute()\n",
    "Y_flt = Y.stack(spatial=['height', 'width'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## generating over-complete set of seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First step is to initialize the **seeds**. Recall the parameters:\n",
    "\n",
    "```python\n",
    "param_seeds_init = {\n",
    "    'wnd_size': 2000,\n",
    "    'method': 'rolling',\n",
    "    'stp_size': 1000,\n",
    "    'nchunk': 100,\n",
    "    'max_wnd': 10}\n",
    "```\n",
    "\n",
    "The idea is that we select some subset of frames, and compute a max projection of those frames, then find the local maximums of that max projection. We keep repeating this process and putting together all the local maximums we get along the way, until we get an overly-complete set of local maximums/bright-spots, which are the potential location of cells and we call them **seeds**. The assumption here is that the center of cells have to be brighter than it's surroundings on some, but not necessarily all, frames. The first and only required argument `seeds_init` takes is `varr` -- the video array we want to process. There are four additional arguments controlling how we subset the frames: `wnd_size` controls the window size of each chunk, *i.e* the number of frames in each chunk; `method` can be either `'rolling'` or `'random'`. For `method='rolling'`, the moving window will roll along `frame`, whereas for `method='random'`, chunks with `wnd_size` number of frames will be randomly pick; `stp_size` is only used if `method='rolling'`, it is the step-size of the rolling windows, in other words the distance between the **center** of each rolling window. For example, if `wnd_size=100` and `stp_size=200`, the windows will be like: **(0, 100)**, **(200, 300)**, **(400, 500)** *etc.* (Obviously that was a **bad** choice since you probably want the windows to overlap otherwise you will miss cells); `nchunk` is only used if `method='random'`, It is the number of random chunks we will draw. \n",
    "<div class=\"alert alert-info\">  \n",
    "The default values of `seeds_init` usually works fairly well for dense region like CA1. If you are working with deep brain region with sparse cells, try to increase wnd_size and stp_size to make following <strong>seeds</strong> cleaning steps faster and cleaner.\n",
    "</div> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "seeds = seeds_init(Y, **param_seeds_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the seeds as points overlaid on top of the `max_proj` image. Each white dot is a seed that being generated and could potentially be the location of a cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_seeds(max_proj, seeds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gaussian-mixture-model (gmm) refine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now come the first step of cleaning the seeds -- by Gaussian Mixture Model. Recall the parameters:\n",
    "\n",
    "```python\n",
    "param_gmm_refine = {\n",
    "    'q': (0.1, 99.9),\n",
    "    'n_components': 2,\n",
    "    'valid_components': 1,\n",
    "    'mean_mask': True}\n",
    "```\n",
    "\n",
    "The idea is that for each seed we have, we calculate the difference between the peak and valley of the fluorecence signal corresponding to the seed pixel. If we put those peak-to-valley values for all the seeds together and look at the histogram of those values, they are supposed to be composed of two Gaussian distribution. We then fit a Gaussian Mixture Model of two components to those peak-to-valley values, and ask the model to predict for each value which Gaussian distribution it is likely to come from. We then keep only the seeds whose peak-to-valley value come from the Gaussian distribution with higher mean, that is, relatively large among all seeds. Thus the assumption here is that of all the \"bright\" seeds we have pulled out from the video, some of them barely changed their fluoresence across the whole recording session, while others changed a lot. Those who changes are healthy active cells whereas those who don't are weird random bright-spots or cells that express fluorecence all the time. \n",
    "\n",
    "`gmm_refine` takes in the video array `varr`, the `seeds` to be refined, and an optional arguments: `q` control how we define \"peak\" and \"valley\". By default `q=(0.1, 99.9)` meaning we use **0.1** percentile as the \"valley\" and **99.9** percentile as the \"peak\"; `n_components` and `valide_components` collectively determine how gaussian model are fit and selected: `n_components` is the number of components/gaussian kernel, and `valid_components` is the number of gaussian kernels that is considered the \"signal\" gaussian -- they are counted from the right, that is, they always have higher mean. `mean_mask` determine whether an additional criteria should be used to select seeds: in addition to being predicted to belong to the \"signal\" gaussian kernels, the peak-to-valley values of the seeds also has to be higher than the smallest mean of all gaussian kernels. This is helpful especially when `n_components` is low but you have different types of trash seeds, and sometimes since the majority of trash seeds compose a really narrow gaussian, the few outlier trash seeds that has very low peak-to-valley value might be falsely classified as signals. \n",
    "<div class=\"alert alert-info\">  \n",
    "Usually it is fine to leave them as-is, but feel free to change them if you believe they are messing things up. Note that it can be speculated that if a cell was firing all the time during recording, they might be filtered out during this step. In that case you might want to skip this step. On the other hand, cell that is on all the time can also be toxic because of accumulating GCaMP protein induced toxicity <i>in vivo</i>, in which case you may want to use this step to clean them out. Make your own judgement about what you want to do here!  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "seeds_gmm, pv, gmm= gmm_refine(Y_flt, seeds, **param_gmm_refine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualize the fit of gaussian models overlaid on top of a histogram of all peak-to-valley values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_gmm_fit(pv, gmm, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again we can visualize the seeds on top of `max_proj`. Here we can color the seeds based on whether they are accepted during `gmm_refine` -- white dots are accepted seeds and red dots are taken out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_seeds(max_proj, seeds_gmm, 'mask_gmm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "If you see seeds that you believe should be cells have been taken out here, feel free to skip this step. \n",
    " \n",
    "\n",
    "Besides, ploting out the trace of specific seed could also be a useful method for you to know if there is anything going wrong. First of all, find out the `height` and `width` infomation of the seed you want to plot trace out using the previous visualization result. For example, if you are interested in a seed locate at `height` = 100, `width` = 200, try the following:\n",
    "```python\n",
    "trace1 = Y.sel(height=100, width=200).values\n",
    "datashade(hv.Curve(trace1)).opts(plot=dict(width=1000))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## peak-noise-ratio refine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we reached the most powerful and important aspect of this pipeline -- parameter exploring. It seem to be the most complicated chunk of code so far, but it is very important to read through it, since we will see similar things later and it is a very powerful piece of code that can help you visualize a lot of stuff. The basic idea is we run some function on a small subset of data using different parameters within a `for` loop, and after that visualize the result using `holoviews`. Note that interactive mode need to be set as `True` for the parameter exploring steps to work.\n",
    "\n",
    "The goal of this specific piece of code is to determine a \"frequency\" at which we can best seperate our signal from noise, which is an important parameter used at various places below. We will go line by line: First we create a `list` of frequencies we want to try out -- `noise_freq_list`. The \"frequency\" values here is a proportion of your **sampling rate**. Then we randomly selected 6 seeds from `seeds_gmm` and call them `example_seeds`, which in turn help us pull out the temporal traces from the movie `Y_flt`, which are assigned to `example_trace`. We then create an empty dictionary `smooth_dict` to store the resulting visualizations. Then there is the `for` loop iterating through `noise_freq_list`, with one of the values from the list as `freq` during each iteration. Within the loop, we run `smooth_sig` twice on `example_trace` with the current `freq` we are testing out. The low-passed result is assigned to `trace_smth_low`, while the high-pass result is assigned to `trace_smth_high`. Then we make sure to actually carry-out the computation by calling the `compute` method on the resulting `DataArray`s. Finally, we turn the two traces into visualizations: we construct [hv.Curve](http://holoviews.org/reference/elements/bokeh/Curve.html)s from them and put them in an container called [hv.HoloMap](http://holoviews.org/reference/containers/bokeh/HoloMap.html). Again if you are confused about how the visualization works, it's time to check out [the tutorial](http://holoviews.org/getting_started/Introduction.html). After that we store the whole visualization into `smooth_dict`, with the keys being the `freq` and values corresponding to the result of this iteration.\n",
    "\n",
    "<div class=\"alert alert-info\"> \n",
    "Here you can edit the values that you want to test in the noise_freq_list.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if interactive:\n",
    "    noise_freq_list = [0.005, 0.01, 0.02, 0.06, 0.1, 0.2, 0.3, 0.45]\n",
    "    example_seeds = seeds_gmm[seeds_gmm['mask_gmm']].sample(6, axis='rows')\n",
    "    example_trace = (Y_flt\n",
    "                     .sel(spatial=[tuple(hw) for hw in example_seeds[['height', 'width']].values])\n",
    "                     .assign_coords(spatial=np.arange(6)))\n",
    "    smooth_dict = dict()\n",
    "    for freq in noise_freq_list:\n",
    "        trace_smth_low = smooth_sig(example_trace, freq)\n",
    "        trace_smth_high = smooth_sig(example_trace, freq, btype='high')\n",
    "        trace_smth_low = trace_smth_low.compute()\n",
    "        trace_smth_high = trace_smth_high.compute()\n",
    "        hv_trace = hv.HoloMap({\n",
    "            'signal': hv.Dataset(trace_smth_low).to(hv.Curve, kdims=['frame']),\n",
    "            'noise': hv.Dataset(trace_smth_high).to(hv.Curve, kdims=['frame'])\n",
    "        }, kdims='trace').collate()\n",
    "        smooth_dict[freq] = hv_trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all the loops are done, we generate again put together a `hv.HoloMap` from `smooth_dict`, and we specify we want the plots to `overlay` with each other along the `'trace'` dimension while laid out along the `'spatial'` dimension. The result turns into a nicely animated interactive plot, from which we can determine the frequency that best separate noise and signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    hv_res = (hv.HoloMap(smooth_dict, kdims=['noise_freq']).collate().opts(plot=dict(height=400, width=600))\n",
    "              .overlay('trace').layout('spatial').cols(3))\n",
    "    display(hv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all these we move on the next step of seeds refining. Recall the parameters:\n",
    "\n",
    "```python\n",
    "param_pnr_refine = {\n",
    "    'noise_freq': 0.06,\n",
    "    'thres': 'auto'\n",
    "    'med_wnd': None}\n",
    "```\n",
    "\n",
    "`pnr_refine` stands for \"peak-to-noise ratio\" refine. The \"peak\" and \"noise\" here are defined differently from before. First we seperate/filter the temporal signal for each seed based on frequency -- the signals composed of the lower half of the frequency are regarded as **real** signals, while the higher half of the frequency are presumably **noise** (\"half\" being relative to [Nyquist frequency](https://en.wikipedia.org/wiki/Nyquist_frequency)). Then we take the peak-to-valley value (really just **max** minus **min**, or, [np.ptp](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.ptp.html)) for both the **real** signal and **noise** signal. Then finally, \"peak-to-noise ratio\" is simply the ratio between the `np.ptp` values of **real** and **noise** signal. So, the critical assumption here is that real activities are lower frequency while noise are higher frequency, and they seperate at approximately half the Nyquist frequency, or, one-fourth of the sampling frequency of the video, and we don't want those \"seeds\" whose **real** signals are buried in **noise**. If these assumptions does not suit your case, for example, if you have a really low sampling rate, or if your video are unavoidably noisy, consider skipping this step. As always, `pnr_refine` takes in `varr` and `seeds` as first two arguments, then the `noise_freq` that best separate signal and noise, which hopefully has been determined from previous cells, and a `thres`, where seeds whose \"peak-to-noise ratio\" fall below this threshold will be discarded. Pragmatically `thres=1` works fine and makes sense. You can also use `thres='auto'`, where again a gaussian mixture model of 2 components will be runned on the peak-to-noise ratios and seeds will be selected if they belong to the \"higher\" gaussian. `med_wnd` is the window size of the median filter that gets passed in as `size` in [`scipy.ndimage.filters.median_filter`](https://docs.scipy.org/doc/scipy-0.16.1/reference/generated/scipy.ndimage.filters.median_filter.html). Normally, when you are working with dataset that with relatively good signal and dense cell populations,  `med_wnd` = None works pretty well. However, it is recommended to use a median filter here if you are dealing with lower quality recording that has higher noise level. Pramatically, `med_wnd` = 501 works well. \n",
    "\n",
    "Now we can use the previous visualization result to pick the best frequency!\n",
    "\n",
    "![pnr_param](pnr_param_v2.png)\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "What we are looking for here is the frequency that can seperate <strong>real</strong> and <strong>noise</strong> signal the best, which means the left panel in the example trace, with the `noise_freq` = 0.005, is not ideal. In the mean time, we also don't want the signal bands to be overly thick which is showing in the right panel with the `noise_freq` = 0.45. Thus, the middle trace with `noise_freq` = 0.05 best suits the needs! \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">  \n",
    "Now, say you already found your parameters, it's time now to pass them in! Either go back to initial parameters setting step and modify them there, or call the parameter here, then change the values accordingly. \n",
    "</div>\n",
    "\n",
    "For example, if you want to change `noise_freq` to 0.05, and start using median filter here as the window size equals 501:\n",
    "```python\n",
    "param_pnr_refine['noise_freq'] = 0.05\n",
    "param_pnr_refine['med_wnd'] = 501\n",
    "```\n",
    "Finally, run the following code cell to further clean the seeds:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds_pnr, pnr, gmm = pnr_refine(Y_flt, seeds_gmm[seeds_gmm['mask_gmm']].copy(), **param_pnr_refine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here in the belowing code cell we will visualize the gmm fit, but **only** when you chose `thres='auto'` before. The x axis here is pnr ratio value, and the x value of the intersection of blue and red curve is the auto chose threshold, everything below this threshold will be seen as noise.\n",
    "If you are using a manully set threshold instead of using `auto`, but you do want to visuaize the histogram of pnr values, here is what you can try:\n",
    "```python\n",
    "hv.Histogram(np.histogram(pnr.values, bins=50))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if gmm:\n",
    "    display(visualize_gmm_fit(pnr, gmm, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again we can visualize seeds that's taken out during this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_seeds(max_proj, seeds_pnr, 'mask_pnr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, white dots are accepted seeds and red dots are taken out. \n",
    "<div class=\"alert alert-info\"> \n",
    "if you see seeds that you believe should be cells have been taken out here, either skip this step or try lower the threshold a bit. Alternatively, raise up the threshold if you see white dots who are supposed to be taken out but haven't. You can also use the individual trace ploting method we discussed at the end of gmm_refine part to look into specific seed. \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intensity refine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`intensity_refine` threshold the seeds based purely on the intensity, and here is how the threshold is found: We first calculate a max projection across all frames, literally `varr.max('frame')`. Then, we bin the max projection so that each bin has [around](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.around.html) 10 pixels. We then use those bins to generate a [histogram](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.histogram.html), and find the bin that has the highest frequency/density. Then the threshold is simply twice the intensity that correspond to the highest bin. `intensity_refine` only takes `varr` and `seeds` with no additional arguments. It can, however, potentially run into problems: In case you have a field-of-view that is bright overall, the threshold might simply be out of the range of the intensities. In that case `intensity_refine` simply do nothing (and it will tell you if that happens). Pragamatically that rarely happens and `intensity_refine` usually does not take away real cells. \n",
    "\n",
    "However if you found this step too arbitrary you should be able to safely skip this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "seeds_int = intensity_refine(max_proj, seeds_pnr[seeds_pnr['mask_pnr']], thres_mul=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_seeds(max_proj, seeds_int, 'mask_int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ks refine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ks_refine` refine the seeds using [Kolmogorov-Smirnov test](https://en.wikipedia.org/wiki/Kolmogorov–Smirnov_test). Recall the parameters:\n",
    "\n",
    "```python\n",
    "param_ks_refine = {\n",
    "    'sig': 0.05}\n",
    "```\n",
    "\n",
    "The idea is simple: for each seeds, their fluorescence level on all frames should be somewhat [bimodal](https://en.wikipedia.org/wiki/Multimodal_distribution), with a large normal distribution representing silence/little activities, and maybe a tiny peak representing when the seed/cell is firing. Thus we can carry out KS test for all seeds against the normal distribution, and keep only the seeds where the null hypothesis (that the fluoresence is simply a normal distribution) is rejected and is defaulted to **0.05**. `ks_refine` takes in `varr` and `seeds` as first two arguments, then a `sig` which is the significance level at which the null hypothesis is rejected. \n",
    "<div class=\"alert alert-info\"> \n",
    "Personally I found this step tend to take away real cells when the video is very short (for example, the one that comes with this package under \"./demo_movies\"), probably because the number of \"active\" frames are too small comparing to the majority of the frames. So feel free to skip this step if you encounter the same situation.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "seeds_ks = ks_refine(Y_flt, seeds_int[seeds_int['mask_int']], **param_ks_refine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_seeds(max_proj, seeds_ks, 'mask_ks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the parameters:\n",
    "\n",
    "```python\n",
    "param_seeds_merge = {\n",
    "    'thres_dist': 5,\n",
    "    'thres_corr': 0.7,\n",
    "    'noise_freq': 'envelope'}\n",
    "```\n",
    "\n",
    "`seeds_merge` try to merge seeds together that potentially come from the same cell, based on spatial distance and temporal correlation. Specifically, `thres_dist` is the threshold for euclidean distance between pair of seeds, in pixels; `thres_corr` is the threshold for pearson correlation between pair of seeds. In addition, it's very beneficial to smooth the signals before running the correlation, and again `noise_freq` determines how the smooth should be done. In addition to feeding a number, you can also use `noise_freq='envelope'` so that a hilbert transform will be runned on the temporal traces of each seeds and the correlation will be calculated on the envelope signal. Any pair of seeds that are within `thres_dist` **and** has a correlation higher than `thres_corr` will be merged together, where \"merge\" meaning only the seed with maximum internsity in the max projection of the video will be kept. Thus `thres_dist` should be the expected size of the cells and `thres_corr` should be relatively high to avoid over-merging.\n",
    "\n",
    "<div class=\"alert alert-info\"> \n",
    "Potentially we could pick out multiple seeds that is actually within one cell, but we want to avoid that as much as possible to have a clean start for CNMF later, you can try lower the thres_corr or raise up the thres_dist to make it clean. Ideally, you would want to see only one accepted seed (white dot) within one cell. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "seeds_final = seeds_gmm[seeds_gmm['mask_gmm']].reset_index(drop=True)\n",
    "seeds_mrg = seeds_merge(Y_flt, seeds_final, **param_seeds_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_seeds(max_proj, seeds_mrg, 'mask_mrg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## initialize spatial and temporal matrices from seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up till now, the seeds we have are only one-pixel dots. In order to kick start CNMF we need something more like the spatial footprint `A` and temporal activities `C` of real cells. Thus we need to `initilalize` `A` and `C` from the `seeds_mrg` we have. Recall the parameters:\n",
    "\n",
    "```python\n",
    "param_initialize = {\n",
    "    'thres_corr': 0.8,\n",
    "    'wnd': 10}\n",
    "```\n",
    "\n",
    "Here is how it is done: For each seed, the temporal activities of surrounding pixels can be put into a matrix `sur` with dimension `frame` and `spatial`, where `spatial` is the flattened representation of pixel location, or, a [stack](http://xarray.pydata.org/en/stable/generated/xarray.DataArray.stack.html)ed version of `height` and `width`. Let `sd` denote the temporal activity of the seed, we use the dot product of the temporal activities between the seed and the surrounding pixels `sur.dot(sd)` normalized by the [Euclidean norm](https://en.wikipedia.org/wiki/Norm_(mathematics)) of `sur`: [np.linalg.norm(sur)](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.linalg.norm.html), as well as `np.linalg.norm(sd)`, as the spatial footprint `a`. In the form of equation: $$\\mathbf{a} = \\frac{\\mathbf{sur} \\cdot \\mathbf{sd}}{\\Vert{\\mathbf{sur}}\\Vert \\Vert{\\mathbf{sd}}\\Vert}$$ We can think of this as a normalized measurement of how similar the temporal activities of each surrounding pixel is comparing to those of the seed. Then we use the dot product `sur.dot(a)` as the temporal trace for the seed, which is conceptually an \"averaged\" activities of all the pixels surrounding the seed weighted by `a`. After that we simply [concatenate](http://xarray.pydata.org/en/stable/generated/xarray.concat.html) the spatial footprints and temporal traces from different seeds together along a new dimension `unit_id` to get our initial `A` and `C` matrix. Finally we need two more terms: `b` and `f` representing the spatial footprint and temporal dynamic of the **background**, respectively. We find those by simply taking the dot product `A.dot(C)`, which is the activities from our initial units, and subtract that from the original movie `varr`. We use the mean of that remainder across `frame` as `b`, and the mean across `height` and `width` as `f`. `initilaize` takes in `varr` and `seeds` as the first two arguments. Two additional arguments determine how we find the `sur`roundings when calculating `a`. Firstly we certainly do not want to consider every other pixels in the movie as `sur` to a seed, and we just need a hard window where any pixels outside that window are ignored during the calculation of `a`. `wnd` is the half-size of that window. Secondly, not every pixels within the range of `wnd` are equally possible to be part of the cell represented by the seed. We thus put another correlation threshold -- pixels whose temporal activities has a pearson correlation with the seed higher than `thres_corr` will be considered as `sur`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "Y = Y.chunk(dict(frame=-1, height=200, width=200))\n",
    "A, C, b, f = initialize(Y, seeds_mrg[seeds_mrg['mask_mrg']], **param_initialize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we visualize the result of our initialization by plotting a projection of the spatial matrix `A`, a raster of the temporal matrix `C`, as well as background terms `b` and `f`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_opts = dict(plot=dict(height=A.sizes['height'], width=A.sizes['width']), style=dict(cmap='Viridis'))\n",
    "cr_opts = dict(plot=dict(height=A.sizes['height'], width=A.sizes['height'] * 2), style=dict(cmap='Viridis'))\n",
    "(regrid(hv.Image(A.sum('unit_id'), kdims=['width', 'height'])).opts(**im_opts)\n",
    " + regrid(hv.Image(C, kdims=['frame', 'unit_id'])).opts(**cr_opts)\n",
    "  + regrid(hv.Image(b, kdims=['width', 'height'])).opts(**im_opts)\n",
    " + datashade(hv.Curve(f, kdims=['frame'])).opts(**cr_opts)\n",
    ").cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we save the results in the dataset. Note here we change the name of a dimension by doing `rename(unit_id='unit_id_init')`, this is a precaution, since the dimension `unit_id` will likely to change after next section **CNMF** (most likely units will be merged), and there will be conflicts then if we save other variables with dimension `unit_id` that has different coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "minian.close()\n",
    "save_minian(A.rename('A_init').rename(unit_id='unit_id_init'), **param_save_minian)\n",
    "save_minian(C.rename('C_init').rename(unit_id='unit_id_init'), **param_save_minian)\n",
    "save_minian(b.rename('b_init'), **param_save_minian)\n",
    "save_minian(f.rename('f_init'), **param_save_minian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section assume you already have good knowledge about using CNMF as a method of extracting neural activities from the movie. If not, please read [the paper](https://www.sciencedirect.com/science/article/pii/S0896627315010843) before proceeding.\n",
    "\n",
    "As a quick reminder and check, here is the essential idea of CNMF: We believe our movie `Y`, with dimension `height`, `width` and `frame`, can be written in (and thus break down as) the following equation: $$\\mathbf{Y} = \\mathbf{A} \\cdot \\mathbf{C} + \\mathbf{b} \\cdot \\mathbf{f} + \\epsilon$$ where `A` is the spatial footprint of each unit, with dimension `height`, `width` and `unit_id`, and `C` is the temporal activities of each unit, with dimension `unit_id` and `frame`. `b` and `f` are the spatial footpring and temporal activities of some background, and $\\epsilon$ is the noise. Note that strictly speaking, matrix multiplication are usually only defined for two dimensional matrices, but our `A` here has three dimensions. So in fact we are taking the [tensor product](https://en.wikipedia.org/wiki/Tensor_product) of `A` and `C`, reducing the dimension `unit_id`. This might seem to complicate things (comparing to just treat `height` and `width` as one flattened `spatial` dimension), but I believe it help you understand the essence of why the equation make sense: when you take a dot product of any two \"matrices\" on certain **dimension**, all that is happening is a **product** followed by a **sum** -- you take the product for all pairs of matching numbers coming from the two \"matrices\", where \"match\" is defined by their index along the said dimension, and then you take the sum of all those products along the dimension. Thus when we take the tensor product of `A` and `C`, we are actually multiplying all those numbers in dimension `height`, `width` and `frame`, matched by `unit_id`, and then take the sum. Conceptually, for each unit, we are weighting the spatial footprint (`height` and `width`) by the fluorecense of that unit on given `frame`, which is the **product**, and then we are overlaying all units together, which is the **sum**. With that, the equation above is trying to say that our movie is made up of a weighted sum of the spatial footprint and temporal activities of all units, plus some background and noise.\n",
    "\n",
    "Now, there is another rule about `C` that separate it from background and noise, and save it from being just some random matrix that happen to fit well with the data `Y` without having any biological meaning, and that is the second essential idea of CNMF: each \"row\" of `C`, which is the temporal trace for each unit, should be described as an [autoregressive process](https://en.wikipedia.org/wiki/Autoregressive_model) (AR process), with a parameter `p` defining the **order** of the AR process: $$ c(t) = \\sum_{i=0}^{p}\\gamma_i c(t-i) + s(t) + \\epsilon$$ where $c(t)$ is the calcium concentration at time (`frame`) $t$, $s(t)$ is spike/firing rate at time $t$ (what we actually care), and $\\epsilon$ is again some noise. Basicly this equation is trying to say that at any given time $t$, the calcium concentration at that moment $c(t)$ depends on the spike at that moment $s(t)$, as well as its own history up to `p` time-step back $c(t-i)$, scaled by some parameters $\\gamma_i$s, plus some noise $\\epsilon$. Another intuition of this equation come from looking at different `p`s: when `p=0`, calcium concentration is an exact copy of the spiking activities, which is probably not true; when `p=1`, calcium concentration is an exponential decay with instant rise triggered by the spikes; when `p=2`, calcium concentration is an exponential decay that has some rising time; when `p>2`, more convoluted waveforms start to emerge. (all of these with some noise, of course)\n",
    "\n",
    "With that, all CNMF is doing is trying to find the `A` and `C` (along with `b` and `f`) that best describe `Y`. There are two more important practical concerns: Firstly we cannot solve this problem in one shot, we need to iteratively and separately update `A` and `C` to approach the true solution, and we need something to start with (that is what **initilization** section is about). Surprisingly often times 2 iterative steps seem to give us good enough results, but you can always add more iterations (and you should be ablt to easily do that after reading the comments); Secondly, by intuition you may define \"best describe `Y`\" as the results that minimize the noise $\\epsilon$ (or residule, if you will). However we have to control for the [sparsity](https://en.wikipedia.org/wiki/Sparse_matrix) of our results as well, since we do not want every little random pixel that happen to correlate with a cell to be counted as part of the spatial footprint of the cell (non-sparse `A`), nor do we want a tiny spike at every frame trying to explain every noisy peak we observe (non-sparse `C`). Thus the balance between fidelity (minimizing error) and sparsity (minimizing non-zero entries) is an important idea for both the spatial and temporal update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we load in our data from previous steps. The `'unit_id'` renamming lines are just for the precaution metioned at the end of **initialization** section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chk = chunks.copy()\n",
    "chk['unit_id_init'] = chk.pop('unit_id')\n",
    "minian = open_minian(dpath,\n",
    "                     fname=param_save_minian['fname'],\n",
    "                     backend=param_save_minian['backend'],\n",
    "                     chunks=chk)\n",
    "Y = minian['Y']\n",
    "A_init = minian['A_init'].rename(unit_id_init='unit_id')\n",
    "C_init = minian['C_init'].rename(unit_id_init='unit_id')\n",
    "b_init = minian['b_init']\n",
    "f_init = minian['f_init']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## estimate spatial noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we can not just ask the computer to minimize the noise as much as possible, we have to get a sense of how much noise is expected before balancing sparsity and error during the minimizing process. `noise_fft` will calculate the noise power for all pixels using fft. Essentially we compute an fft-transform for every pixel indepedently, which left us with a [power spectral density](https://en.wikipedia.org/wiki/Spectral_density) with dimension `spatial` and `freq`, where `spatial` is flattened `height` and `width`, and `freq` is the dimension representing different frequency component of the signal. The power spectrum returned as `psd` in the following code and we can then visualize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "psd = psd_fft(Y)\n",
    "if interactive and in_memory:\n",
    "    psd = psd.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this we plot the power spectrum `psd`. Note that the range of `psd` are usually wild and it is usually helpful to limit the range of our color with [`redim.range()`](http://holoviews.org/user_guide/Annotating_Data.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%opts Image [height=300, width=800, colorbar=True, logz=True] (cmap='Viridis')\n",
    "if interactive:\n",
    "    psd_flt = psd.stack(spatial=['height', 'width'])\n",
    "    hv_psd = hv.Image(\n",
    "        psd_flt.assign_coords(spatial=range(psd_flt.sizes['spatial'])).rename('psd'),\n",
    "        kdims=['spatial', 'freq'])\n",
    "    display(regrid(hv_psd).redim.range(psd=(0, 5e-3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to define noise with the spectrum. Recall the parameters:\n",
    "\n",
    "```python\n",
    "param_get_noise = {\n",
    "    'noise_range': (0.06, 0.5),\n",
    "    'noise_method': 'logmexp'}\n",
    "```\n",
    "\n",
    "Note that the number in `noise_range` is relative to the sampling frequency. So **0.5** actually represent Nyquist frequency and is the highest you can go as far as fft concerns. Thus **(0.25, 0.5)** is the higher frequency half of the signal. After choosing `noise_range`, we have to decide how to collapse across different `freq` to get a single number of noise power for each pixel. Three `noise_method`s are availabe: `noise_method='mean'` and `noise_method='median'` will use the mean and median across all `freq` as the estimation of noise for each pixel. `noise_method='logmexp'`is a bit more complicated -- the equation goes like: $sn = \\exp( \\operatorname{\\mathbb{E}}[\\log psd] )$ where $\\exp$ is the [exponential function](Exponential_function), $\\operatorname{\\mathbb{E}}$ is the [expectation operator](https://en.wikipedia.org/wiki/Expected_value) (mean), $\\log$ is [natural logarithm](https://en.wikipedia.org/wiki/Natural_logarithm), $psd$ is the spectral density of noise for any pixel and $sn$ is the resulting estimation of noise power. It is recommended to keep `noise_method='logmexp'` since this is the default behavior of the [CaImAn](https://github.com/flatironinstitute/CaImAn) package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sn_spatial = get_noise(psd, **param_get_noise).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test parameters for first spatial update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will soon do some parameter exploring before actually commit to a spatial update, since you do not want to do a 10-minutes' worth step only to find out you started with some rediculous value for certain parameter. Obviously we only want to run the parameter exploring on very small subset of data. So here we randomly select 10 units from `A_init.coords['unit_id']` with the help of [`np.random.choice`](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.random.choice.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    units = np.random.choice(A_init.coords['unit_id'], 10)\n",
    "    units.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again we do parameter exploring using the `for` loop and visualization with help of `dict` and `holoviews`, only this time we use a convenient function `visualize_spatial_update` from `minian` to handle all the visualization detail. For now there is only one parameter in `update_spatial` that we are interested in playing with -- `sparse_penal`, but there is nothing stopping you from adding more. Discussion of all the parameters for `update_spatial` will follow soon.\n",
    "\n",
    "<div class=\"alert alert-info\">  \n",
    "Here, you can simply <strong>add</strong> the values that you want to test or <strong>delete</strong> the values you are not interested in <strong>spar_ls</strong>, pragmatically, [0.05, 1] is a reasonable range of values.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if interactive:\n",
    "    sprs_ls = [0.1, 0.5, 1]\n",
    "    A_dict = dict()\n",
    "    C_dict = dict()\n",
    "    for cur_sprs in sprs_ls:\n",
    "        cur_A, cur_b, cur_C, cur_f = update_spatial(\n",
    "            Y, A_init.sel(unit_id=units),\n",
    "            b_init, C_init.sel(unit_id=units), f_init, sn_spatial, dl_wnd=20, sparse_penal=cur_sprs)\n",
    "        if cur_A.sizes['unit_id']:\n",
    "            A_dict[cur_sprs] = cur_A.compute()\n",
    "            C_dict[cur_sprs] = cur_C.compute()\n",
    "    hv_res = visualize_spatial_update(A_dict, C_dict, kdims=['sparse penalty'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we actually plot the visualization `hv_res`. What you should expect here will be explained later along with what `sparse_penal` actually do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    display(hv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first spatial update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the idea behind `update_spatial`, recall tha parameters:\n",
    "\n",
    "```python\n",
    "param_first_spatial = {\n",
    "    'dl_wnd': 5,\n",
    "    'sparse_penal': 0.1,\n",
    "    'update_background': False,\n",
    "    'post_scal': True,\n",
    "    'zero_thres': 'eps'}\n",
    "```\n",
    "\n",
    "To reiterate, the big picture is that given the data `Y` and `C` from previous update (which is `C_init` for now), we want to find the `A` so that 1. the **error** `Y - A.dot(C, 'unit_id')` is as small as possible, and 2. the [**l1-norm**](http://mathworld.wolfram.com/L1-Norm.html) of `A` is as small as possible. Here the **l1-norm** is a proxy to control for the sparsity of `A`. Ideally to promote sparsity we want to control for the number of non-zero entry in `A`, which is [l0-norm](https://en.wikipedia.org/wiki/Lp_space#When_p_=_0). However optimizing for l0-norm is usually considered [computationally hard to do](https://stats.stackexchange.com/questions/269298/why-do-we-only-see-l-1-and-l-2-regularization-but-not-other-norms), and it is usually good enough to use **l1-norm** instead as a proxy.\n",
    "\n",
    "Now, in theory we want to update every entry in  `A` iteratively with the above two goals in mind. However updating that amount of numbers in `A` is still computationally very demanding, and it is much better if we can breakdown the big problem into smaller chunks so that we can parallelize them. **CNMF** is all about solving the issues caused by overlapping neurons, so it is best to keep the dependency along `unit_id` and update those entries together. However, it is totally fine to treat each pixel as independent and update different pixels separately (in parallel). Thus, our new \"smaller\" problem is: for each pixel, find the corresponding pixel in `A` across all `unit_id` that gives us smallest **l1-norm** as well as smallest **error** when multiplied by `C`. In equation, this is:\n",
    "\n",
    "$$\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{A_{ij}}{\\text{minimize}}\n",
    "& & \\left \\lVert Y_{ij} - A_{ij} \\cdot C \\right \\rVert ^2 + \\alpha \\left \\lvert A_{ij} \\right \\rvert \\\\\n",
    "& \\text{subject to}\n",
    "& & A_{ij} \\geq 0 \n",
    "\\end{aligned}\n",
    "\\end{equation*}$$\n",
    "\n",
    "where we use $A_{ij}$ to represent one pixel in `A` like `A.sel(height=i, width=j)`, which will only have one dimension `unit_id` left. Similarly $Y_{ij}$ is the corresponding pixel in `Y` which only have one dimension `frame` left. Thus $\\left \\lVert Y_{ij} - A_{ij} \\cdot C \\right \\rVert ^2$ is our **error** term and $\\left \\lvert A_{ij} \\right \\rvert$ is our **l1-norm**, and we are finally putting them together as one goal, with an $\\alpha$ controlling the balance between them. So the higher the $\\alpha$, the more contribution the **l1-norm** term make to the common goal (target function), the more penalty/emphasis you are putting on sparsity, and as a result the more sparse `A` would become. The determination of the exact value of $\\alpha$ is rather complicated, but the parameter we have for `update_spatial` is relative, where `alpha=1` correspond to the default behavior of **CaImAn** package, and is usually a good place to start testing. \n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "Here is a good place that we bring back the parameter exploring visualization result and make sense of it! Pragmatically, relatively small value of <strong>sparse_penal</strong> has very little impact on the resulting <strong>A</strong>, but once you hit a large enough value, you will start to see units getting dimmer, sometimes completely disappear when you do the parameter exploring. You might think that is sparsity penalty in action, but from experience this is usually the case you want to <strong>avoid</strong>, since after all <strong>update_spatial</strong> has no way to differentiate noise from cells other than their corresponding temporal trace, thus you do not want it to take out cells for you unless you absolutely trust the temporal traces (which you shouldn't for now since it's the first update and the temporal trace we have are merely some weighted mean of the original movie). If you are still puzzled about how to pick the right <strong>sparse_panel</strong> from the plot you get from the previous parameter exploring step, here we are going to show you an example which might be helpful!\n",
    "</div>    \n",
    "\n",
    "![1st spatial update param exploring](sparse_panel_spatial_update.PNG)\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "What you are seeing here is parameter testing of first spatial update. Left panel is the result under <strong>sparse_penal = 0.01</strong>, while the middle one is under <strong>sparse_penal = 0.3</strong>, and the right on is under <strong>sparse_penal = 1</strong>. Ideally, we would want the <strong>Binary Spatial Matrix</strong> to best mimic the real spatial footprint, which also means, they should shape like a cell. Thus, in this specific example, <strong>sparse_panel = 0.01</strong> (left penal) is not the good choice. Secondly, we also don't want to actually get rid of cells by using a high sparse panelty value at this step, which indicates <strong>sparse_panel = 1</strong> (right penal) is not good as well. Gnerally, <strong>sparse_panel = 0.3</strong> (middle penal) is a fairly good parameter to choose here.\n",
    "</div>\n",
    "\n",
    "There is yet another parameter `dl_wnd` that is related to a practical consideration: recall that we are updating $A_{ij}$ for our \"small\" problem, which has the dimension `unit_id` and has `A.sizes['unit_id]` number of entries in them (that is, the number of units you have). This is computationally feasible, but still a lot, especially when you do this to all the pixels. One way to reduce the computational demands is to leave out certain units when updating certain pixels -- it does not make sense to consider a unit that is supposed to be at the top left corner of the field of view when we update a pixel in bottom right corner. In other words, for each pixel, we solve the \"small\" problem with only a subset of all the potential units, thus hugely increase the speed of `update_spatial`. This is where `A_init` comes into play (actually the only place it is used -- we do not need `A` at all for the update itself). We compute a morphological dilation that is used during [background removal](#background-removal) on `A_init`, unit by unit, with window size `dl_wnd`, and we use the result as a **masking matrix**, so that during the actual update of any given pixel, only the unit that has a non-zero value at the corresponding pixel in the **masking matrix** will be considered for updating. In other words, we are allowing each unit to expand from `A_init` up to a distance of `dl_wnd`, and kill off any possibility beyond that range, since they will not be considered for update with those pixels outside the **masking matrix**. The rationale of using `dl_wnd` here is that even, for some reason, we have only one non-zero pixel representing the center of certain unit in `A_init`, that unit can potentially expand to a full size cell, but anything beyond that would probably be either part of other cell or random noise. Thus, we want to set this limit here to get the footprint clean.\n",
    "\n",
    "Then we have a boolean parameter `update_background` controlling whether we want to update the background in this step. This is the only place in the pipeline that the background will be updated, and essentially the way it is updated is simply treating `b` as another `unit` and update it according to the temporal activity `f`. Pragmatically since the morphology-based [background removal](#backgroun-removal) works so well at cleaning the backgrounds, this updating has little impact on the result.\n",
    "\n",
    "Due to the actual implementation of the optimization method, it is hard for the computer to set some variable to absolutely zero. Instead we usually have a very small float number at the place that should be zero. `zero_thres` solve this by thresholding all the values and set anything below `zero_thres` to zero. Apparently you want to use a very small number for `zero_thres`. Setting `zero_thres='eps'` will use the [machine epsilon](https://en.wikipedia.org/wiki/Machine_epsilon)(the smallest non-negative number a machine can represent) of current datatype.\n",
    "\n",
    "Finally we have an additional step after everything: a post-hoc scaling. The rationale for this is that since we are using **l1-norm** as one of the term to be minimized, we can mistakenly reduce the amplitude of some number to a very small extend. Thus we try to scale them back after the optimization step by assigning a single scaling factor to each `unit`, and the values of scaling factors are found simply by least-square when regressing the scaled version of `A` with the original `Y` averaged across `frame`. The boolean parameter `post_scal` controls whether this scaling process should be done.\n",
    "\n",
    "`update_spatial` takes in the original data `Y`, the initial spatial footprint for units and background `A` and `b` respectively, the initial temporal trace for units and background `C` and `f` respectively, and the estimated noise on each pixel `sn`, in that order. Optional arguments are `sparse_penal`, `dl_wnd`, `update_background`, `post_scal` and `zero_thres` which we have already discussed.\n",
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "In all, we find most of the time, the two most useful parameters you might want to modify here are <strong>sparse_penal</strong> and <strong>dl_wnd</strong>. Pass in the new sparse_penal in the dictionary of <strong>param_first_spatial</strong>. For <strong>dl_wnd</strong>, We can again use the expected size of cell as <strong>dl_wnd</strong>. If you find you units being cut-off abruptly and seem to be not able to expand enough, try increasing <strong>dl_wnd</strong>. If you do not like this feature and feel happy with the full scale computation, setting <strong>dl_wnd=None</strong> or <strong>dl_wnd=0</strong> will turn off the masking completely.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "A_spatial, b_spatial, C_spatial, f_spatial = update_spatial(\n",
    "    Y, A_init, b_init, C_init, f_init, sn_spatial, **param_first_spatial)\n",
    "A_spatial = A_spatial.chunk(dict(unit_id = chunks['unit_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "opts = dict(plot=dict(height=A_init.sizes['height'], width=A_init.sizes['width'], colorbar=True), style=dict(cmap='Viridis'))\n",
    "(regrid(hv.Image(A_init.sum('unit_id').compute().rename('A'), kdims=['width', 'height'])).opts(**opts).relabel(\"Spatial Footprints Initial\")\n",
    "+ regrid(hv.Image((A_init.fillna(0) > 0).sum('unit_id').compute().rename('A'), kdims=['width', 'height']), aggregator='max').opts(**opts).relabel(\"Binary Spatial Footprints Initial\")\n",
    "+ regrid(hv.Image(A_spatial.sum('unit_id').compute().rename('A'), kdims=['width', 'height'])).opts(**opts).relabel(\"Spatial Footprints First Update\")\n",
    "+ regrid(hv.Image((A_spatial > 0).sum('unit_id').compute().rename('A'), kdims=['width', 'height']), aggregator='max').opts(**opts).relabel(\"Binary Spatial Footprints First Update\")).cols(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "opts_im = dict(plot=dict(height=b_init.sizes['height'], width=b_init.sizes['width'], colorbar=True), style=dict(cmap='Viridis'))\n",
    "opts_cr = dict(plot=dict(height=b_init.sizes['height'], width=b_init.sizes['height'] * 2))\n",
    "(regrid(hv.Image(b_init, kdims=['width', 'height'])).opts(**opts_im).relabel('Background Spatial Initial')\n",
    " + datashade(hv.Curve(f_init, kdims=['frame'])).opts(**opts_cr).relabel('Background Temporal Initial')\n",
    " + regrid(hv.Image(b_spatial, kdims=['width', 'height'])).opts(**opts_im).relabel('Background Spatial First Update')\n",
    " + datashade(hv.Curve(f_spatial, kdims=['frame'])).opts(**opts_cr).relabel('Background Temporal First Update')\n",
    ").cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test parameters for first temporal update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First off we select some `units` to do parameter exploring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    units = np.random.choice(A_spatial.coords['unit_id'], 10, replace=False)\n",
    "    units.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we move on to the parameter exploring of temporal update. Here we use the same idea we have before, only this time there is much more parameters to play with for temporal update, and we now have four `list`s of potential parameters: `p_ls`, `sprs_ls`, `add_ls`, and `noise_ls`. We use [`itertools.product`](https://docs.python.org/3.7/library/itertools.html#itertools.product) to iterate through all possible combinations of the potential values and save us from nested `for` loops.\n",
    "\n",
    "<div class=\"alert alert-info\">  \n",
    "Here, you can simply <strong>add</strong> the values that you want to test or <strong>delete</strong> the values you are not interested in <strong>p_ls</strong>, <strong>sprs_ls</strong>, <strong>add_ls</strong>, <strong>noise_ls</strong>. pragmatically, to save of your time, we want to provide some reference testing range of these parameters: <strong>p_ls</strong>: from 1 to 3;  <strong>spar_ls</strong>: from 1 to 20; <strong>add_ls</strong>: from 10 to 20; <strong>noise_ls</strong>: from 0.02 to 0.1. Note that this is <strong>just</strong> suggestions we offered to save your time on testing parameters too out of range. But feel free to expore your own parameters here!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if interactive:\n",
    "    p_ls = [2]\n",
    "    sprs_ls = [1, 3, 5]\n",
    "    add_ls = [20]\n",
    "    noise_ls = [0.06]\n",
    "    YA_dict, C_dict, S_dict, g_dict, sig_dict, A_dict = [dict() for _ in range(6)]\n",
    "    for cur_p, cur_sprs, cur_add, cur_noise in itt.product(p_ls, sprs_ls, add_ls, noise_ls):\n",
    "        ks = (cur_p, cur_sprs, cur_add, cur_noise)\n",
    "        print(\"p:{}, sparse penalty:{}, additional lag:{}, noise frequency:{}\"\n",
    "              .format(cur_p, cur_sprs, cur_add, cur_noise))\n",
    "        YrA, cur_C, cur_S, cur_B, cur_C0, cur_sig, cur_g, cur_scal = update_temporal(\n",
    "            Y, A_spatial.sel(unit_id=units), b_spatial, C_spatial.sel(unit_id=units),\n",
    "            f_spatial, sn_spatial, sparse_penal=cur_sprs, p=cur_p, use_spatial=False, use_smooth=True,\n",
    "            add_lag = cur_add, noise_freq=cur_noise, cvx_sched=\"processes\", chk=chunks)\n",
    "        cur_A = A_spatial.sel(unit_id = cur_C.coords['unit_id'])\n",
    "        YA_dict[ks], C_dict[ks], S_dict[ks], g_dict[ks], sig_dict[ks], A_dict[ks] = (\n",
    "            YrA, cur_C, cur_S, cur_g, cur_sig, cur_A)\n",
    "    hv_res = visualize_temporal_update(\n",
    "        YA_dict, C_dict, S_dict, g_dict, sig_dict, A_dict,\n",
    "        kdims=['p', 'sparse penalty', 'additional lag', 'noise frequency'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A piece of useful infomation after you run this cell is that under what testing parameter, which sample units got dropped because of poor fit:\n",
    "![dropped sample units](img/first_tem_drop_v2.PNG)\n",
    "<div class=\"alert alert-success\">  \n",
    "Cross compare this with the raw trace plot, find the most reasonable parameters that drop the right sample cells.\n",
    "</div>\n",
    "\n",
    "Then,  we plot the visualization `hv_res` of the 10 ramdom units we just generated at the belowing code cell. Don't worry if each parameter doesn't make much sense now, What you should expect here will be explained later in <strong>first temporal update</strong> along with what `param_first_temporal` actually does (Look for the green tips box)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    display(hv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## first temporal update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the idea for temporal update: Recall tha parameters:\n",
    "\n",
    "```python\n",
    "param_first_temporal = {\n",
    "    'noise_freq': 0.06,\n",
    "    'sparse_penal': 1,\n",
    "    'p': 2,\n",
    "    'add_lag': 20,\n",
    "    'use_spatial': False,\n",
    "    'chk': chunks,\n",
    "    'jac_thres': 0.1,\n",
    "    'zero_thres': 1e-8,\n",
    "    'max_iters': 200,\n",
    "    'use_smooth': True,\n",
    "    'scs_fallback': False,\n",
    "    'post_scal': True}\n",
    "```\n",
    "\n",
    "Similar to the spatial update, given  the spatial footprint of each unit (`A`), our goal is now to find the activity of each unit (`C`) that minimizes both the **error** (`Y - A.dot(C, 'unit_id')`) and the **l1-norm** of `C`. However there is an additional constraint: the trace of each unit in `C` must follow an autoregressive process. Due to this additional layer of complexity, things becomes more computationaly expensive.  To reduce computatioinal cost, first observe that `A` is usually much larger than `C` (you usually have more total pixels than `frame`s), and performing the dot product, `A.dot(C, 'unit_id')`, everytime you try a different number in `C`,  is infeasible. Thus, we convert our **error** term to something like $\\mathbf{A}^{-1} \\cdot \\mathbf{Y} - \\mathbf{C}$, where $\\mathbf{A}^{-1}$ represents a matrix that can \"undo\" what `A` usually does to `C` -- instead of weighting the temporal activity of each unit by its spatial footprint (converting a matrix with dimension `unit_id` and `frame` into one with dimensions `height`, `width` and `frame`), $\\mathbf{A}^{-1}$ \"extracts\" the temporal activity of each unit based upon their spatial footprint (converting a matrix with dimension `height`, `width` and `frame` into one with dimensions `unit_id` and `frame`). In other words, $\\mathbf{A}^{-1}$ is like an [inverse](https://en.wikipedia.org/wiki/Moore–Penrose_inverse) of `A`. This way, we only need to calculate $\\mathbf{A}^{-1} \\cdot \\mathbf{Y}$ once and be done -- we can use that result everytime we update `C`. The calculation of $\\mathbf{A}^{-1} \\cdot \\mathbf{Y}$ is rather complicated and not strictly mathematically accurate, but it provides a good approximation with huge computational benefit, and is the default behavior of CaImAn. You can turn this off by supplying `use_spatial=True` -- however that is usually too computationally demanding to do. We will assume `use_spatial=False` in the following discussion and call the $\\mathbf{A}^{-1} \\cdot \\mathbf{Y}$ term `YrA`, as in the code. The second thing to observe is that we cannot keep the `unit_id` dimension and chop up the `frame` dimension for parallel processing (like how we chopped up pixels during the spatial update), since we have to check whether each trace along the `frame` dimension follows an autoregressive process. Instead, we turn to the `unit_id` dimension to make our problem \"smaller\". Since we have a relatively good `A` now, it should be OK to update units that are not spatially overlapping independently. This idea should work if you have a relatively sparse distribution of cells. However if your field-of-view is packed with cells, if we were to consider cells overlapping if they share only one pixel, we would likely end up having to update `C` altogether, since every cell is transitively overlapping with every other cell. Instead, we put a threshold on how we define \"overlap\", and that is what `jac_thres` is for -- only cells that have an area of their spatial footprint overlapping that is more than this threshold (ranging from 0 to 1) will be considered \"overlapping\". (The \"proportion of overlapping area\" has a formal name: [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index), hence the name `jac_thres`). Pragamatically `jac_thres=0.2` works for data that is very compact in cells.\n",
    "\n",
    "We now turn to the \"other layer of complexity,\" which is the autoregressive process. Recall that the temporal trace of each unit should be fitted by the following equation: $$c(t) = \\sum_{i=0}^{p}\\gamma_i c(t-i) + s(t) + \\epsilon$$ The first thing we want to determine is `p`. As discussed before, `p=2` is a good choice if your calcium transients have an observable rise-time. `p=1` might work better if the rise-time of your signal is faster than your sampling rate and you thus don't need to explicitly model it. Notably, `p>2` could result in [over-fitting](https://en.wikipedia.org/wiki/Overfitting) and is not recomended unless you are certain that your calcium traces have a more complicated waveform. Next, notice that we have several $\\gamma_i$s unaccounted for (though usually not too many if `p` is small). Luckily, we do not have to iteratively update these -- it turns out that the $\\gamma_i$s of an autoregressive process are related to the [autocovariance](https://en.wikipedia.org/wiki/Autocovariance) of the signal at different lags, which can be readily computed from `YrA`. For full derivation of these relationships, please refer to the [original CNMF paper](https://www.sciencedirect.com/science/article/pii/S0896627315010843?via%3Dihub). Here, we will merely assume that the parameters that affect how much a signal depends on its own history are related to the covariance of the signal when you shift it by different temporal lags. In this way, $\\gamma_i$s can be computed rather deterministicly. Say you set `p=2` and thus you have two $\\gamma_i$s to be estimated -- you would need exactly two equations involving the autocovariance function up to 2 time-step lags to give you the two $\\gamma_i$s. However, you can add additional equations using different lags to better model the propogation of signal, since the impact of $\\gamma_i$s can theoretically extend infinitely back in time, and should be reflected in the autocovariance function at any additional lag. In practice, we use a finite number of equations, solved with [least squares](https://en.wikipedia.org/wiki/Least_squares). Thus it is important to choose an appropriate number of **additional** equations, which is what `add_lag` controls. An `add_lag` that is too small like `add_lag=0` will leave everything to the first `p` number of equations and autocovariance functions, which might not be reliable. Pragmatically, smaller `add_lag` values tend to bias the $\\gamma_i$s to give a much faster decay, whereas larger `add_lag` values tend to give a longer decay. **As a rule of thumb, it is usually good to set `add_lag` to approximately the decay time of your signal (in frames).** \n",
    "\n",
    "Once we have estimated the $\\gamma_i$s, the calcium traces, $c(t)$, and spikes, $s(t)$, are essentially **one thing** -- given calcium traces and how they rise/decay in response to spikes, we can deduce where the spikes happen, and *vice versa*. We can express this determined relationship with a matrix $\\mathbf{G}$ where $s(t) = \\mathbf{G} \\cdot c(t)$. In other words, $\\mathbf{G}$ is the matrix that \"undoes\" what $\\gamma_i$s do to $s(t)$. With all these parameters sorted out, we finally come to the actual optimization problem:\n",
    "\n",
    "$$\\begin{equation*}\n",
    "\\begin{aligned}\n",
    "& \\underset{C_{i}}{\\text{minimize}}\n",
    "& & \\left \\lVert \\mathbf{YrA}_{i} - \\mathbf{C}_{i} \\right \\rVert ^2 + \\alpha \\left \\lvert \\mathbf{G}_{i} \\cdot \\mathbf{C}_{i} \\right \\rvert \\\\\n",
    "& \\text{subject to}\n",
    "& & \\mathbf{C}_{i} \\geq 0, \\; \\mathbf{G}_{i} \\cdot \\mathbf{C}_{i} \\geq 0 \n",
    "\\end{aligned}\n",
    "\\end{equation*}$$\n",
    "\n",
    "Just as during the spatial update, we select some units ($i$), and update their calcium dynamics ($\\mathbf{C}_i$) based on the **error** and the **l1-norm** of the **spikes** ($\\mathbf{G}_i \\cdot \\mathbf{C}_i$). Again, it does not make sense to have negative calcium dynamics or spikes, so that is a constraint on the problem. Moreover, we need an $\\alpha$ to provide balance between fidelity and sparsity, which can be scaled up and down with `sparse_penal` (`sparse_penal=1` is equivalent to the default behavior of CaImAn). Furthermore, $\\alpha$ should depend on the expected level of noise. Note that we cannot use `sn_spatial` since that was the noise for each pixel, and we need the noise for each unit. The function `update_temporal` estimates the noise of each unit for you -- you just have to tell it the `noise_freq`uency. Like before, **0.5** is the highest you can go. With the default, `noise_freq=0.25`, the higher frequency half of the signal will be considered noise. In addition to affecting the estimation of noise power, `noise_freq` affects another smoothing process: when estimating $\\gamma_i$s, it is usually helpful to run a filter on the signal to get rid of high freqeuency noise, particularly when you don't have a large `add_lag`. The parameter, `noise_freq` is the cut-off frequency of the low-pass filter run on the temporal trace for each unit.  Additionally, you can set the value of `use_smooth` to control whether the filtering is done at all. Even with this careful design, however, it is sometimes hard to approach the true solution to the problem. When that happens, `update_temporal` will warn you by saying something like \"problem solved sub-optimally\". Usually, a few of these warnings is OK, but if you see this warning a lot it either means your parameters are unreasonable or you need more iterations to approach the real answer. You can use `max_iters` to control how many iterations to run for each small problem before the computer gives up and throws a warning. Furthermore, in some very, very rare cases, the default [ecos solver](https://www.cvxpy.org/tutorial/advanced/index.html#choosing-a-solver) (the algorithm that does all the heavy-lifting) can fail and throw a \"problem infeasible\" warning, and it's worth trying a different solver, namely [scs](https://www.cvxpy.org/tutorial/advanced/index.html#choosing-a-solver).  Be aware that scs produces results with very, very slow performance. The boolean parameter `scs_fallback` controls whether the scs attempt should be made before giving up. Importantly, both increasing `max_iters` and using `scs_fallback` will significantly increase the computation time and will not help at all if the parameters you provided are unreasonable to begin with, so try to use this only as a last resort.\n",
    "\n",
    "Finally, after the optimization is done, and just like [`update_spatial`](#first-spatial-update), we have a `zero_thres` to get rid of the small numbers, after which we can do a `post_scal` to counter the artifacts introduced by the **l1-norm** penalty.\n",
    "\n",
    "`update_temporal` takes in `Y`, `A`, `b`, `C`, `f`, and `sn_spatial` (even if we won't need it by default), in that order. Optionally you can pass in `noise_freq`, `p`, `add_lag`, `jac_thres`, `use_spatial`, `sparse_penal`, `max_iters`, `use_smooth`, `scs_fallback`, `zero_thres` and `post_scal`, as we have discussed. `update_temporal` returns much more than we expected -- in addition to `C_temporal` and `S_temporal`, which are the results we care most about, it also returns `YrA`, and `g_temporal` (the $\\mathbf{G}$ matrix for each unit). Moreover, it returns `B_temporal`, `C0_temporal` and `sig_temporal`, representing the final layer of complexity: when we update the temporal trace, there might be a global baseline calcium concentration, which is modeled by $b$ and returned in `B_temporal`. A spike may also have happened right before recording starts and the resulting calcium transient could still be decaying in the first few seconds, so we model this with an initial calcium concentration, $c_0$, that follows the same decaying pattern defined by $\\gamma_i$s, and is returned in `C0_temporal`. Both $b$ and $c_0$ are single numbers that get updated along with the calcium dynamics for each unit. Finally there is `sig_temporal` which is the combination of all the signals, that is: `C_temporal + C0_temporal + B_temporal`\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "You should now have an idea of what each parameter is doing in `update_temporal`, and be able to make sense of the visualization results of the parameter exploring steps.\n",
    "    \n",
    "\n",
    "- As was briefly mentioned before, minian's output of <strong>dropped sample units</strong> information and visualization of their <strong>raw traces</strong> is useful after the first temporal update. Since one of the main purposes of the <strong>first temporal update</strong> is to get rid of trash cells and cells with noisy signal, successful parameter selection is evidenced by dropped units with raw traces that look like noise (no clear bursts of activity). Alternatively, if cell-like activity is seen in the raw trace of a dropped unit, this may indicate that the selected parameters are too conservative. \n",
    "\n",
    "- When reading the temporal trace plot, \"fitted spikes\" (green), \"fitted signal\" (orange), and \"fitted calcium trace\" (blue), are all alligned to the \"raw signal\" based upon the model. Ideally, we want only one spike for each burst of signal, with \"fitted signal\" and \"fitted calcium trace\" decaying in a manner that follows the raw signal. Below is the temporal plot of an example unit using different <strong>sparse_panel</strong>:\n",
    "\n",
    "### Example Temporal Traces\n",
    "\n",
    "![example temporal traces](first_tem_param.png)\n",
    "\n",
    "\n",
    "\n",
    "Here, the top trace is when <strong>sparse_panel</strong> = 1, and we can see that there are lots of small spikes at the bottom, indicating we may want to increase the <strong>sparse_panel</strong> to get rid of them. However, when we are using <strong>sparse_panel</strong> = 10 (bottom panel), it's clear that we are missing real spikes from raw signal. Thus, the middle panel with <strong>sparse_panel</strong> = 3 fits the raw signal the best here.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below produces plots of temporal traces and spikes after the first temporal update and allows us to compare them to the signal originiating from the initialization step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "YrA, C_temporal, S_temporal, B_temporal, C0_temporal, sig_temporal, g_temporal, scale = update_temporal(\n",
    "    Y, A_spatial,\n",
    "    b_spatial, C_spatial, f_spatial, sn_spatial, **param_first_temporal)\n",
    "A_temporal = (A_spatial.sel(unit_id = C_temporal.coords['unit_id'])\n",
    "              .chunk(chunks['unit_id']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "opts_im = dict(plot=dict(height=500, width=1000, colorbar=True), style=dict(cmap='Viridis'))\n",
    "ranges = dict(c=(0, 1.5), s=(0, 0.04))\n",
    "(regrid(hv.Image(C_init.rename('c'), kdims=['frame', 'unit_id'])).opts(**opts_im).relabel(\"Temporal Trace Initial\").redim.range(**ranges)\n",
    " + hv.Curve([]).opts(xaxis=None, yaxis=None, show_frame=False)\n",
    " + regrid(hv.Image(C_temporal.rename('c'), kdims=['frame', 'unit_id'])).opts(**opts_im).relabel(\"Temporal Trace First Update\").redim.range(**ranges)\n",
    " + regrid(hv.Image(S_temporal.rename('s'), kdims=['frame', 'unit_id'])).opts(**opts_im).relabel(\"Spikes First Update\").redim.range(**ranges)).cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell of code allows us to visualize units that were dropped during the first temporal update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    h, w = A_spatial.sizes['height'], A_spatial.sizes['width']\n",
    "    im_opts = dict(plot=dict(height=h, width=w), style=dict(cmap='Viridis'))\n",
    "    cr_opts = dict(plot=dict(height=h, width=2*w))\n",
    "    bad_units = list(set(A_spatial.coords['unit_id'].values) - set(A_temporal.coords['unit_id'].values))\n",
    "    bad_units.sort()\n",
    "    hv_res = (datashade(hv.Dataset(YrA.sel(unit_id=bad_units).rename('raw'))\n",
    "                        .to(hv.Curve, kdims=['frame'])).opts(**cr_opts).relabel(\"Temporal Trace\")\n",
    "              + hv.Div('')\n",
    "              + regrid(hv.Dataset(A_spatial.sel(unit_id=bad_units).rename('A'))\n",
    "                       .to(hv.Image, kdims=['width', 'height'])).opts(**im_opts).relabel(\"Spatial Footprint\")\n",
    "              + (regrid(hv.Image(A_temporal.sum('unit_id').compute().rename('A'), kdims=['width', 'height']))\n",
    "                 .opts(**im_opts)).relabel(\"Spatial Footprints of Accepted Units\")).cols(2)\n",
    "    display(hv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we can visualize the activity of each unit. There are four traces in the top plot: \"Raw Signal\" corresponds to `YrA`, \"Fitted Spikes\" to `S_temporal`, \"Fitted Calcium Trace\" to `C_temporal` and \"Fitted Signal\" to `sig_temporal`. The latter two traces usually overlap with each other since `B_temporal` and `C0_temporal` are often equal **0**. Sadly, due to large number of frames and the limitation of our browser, it is usually only possible to visualize 50 units at a time, hence `select(unit_id=slice(0, 50))`. Nevertheless it gives us an idea of how things went. Put in other numbers if you want to see other units. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    display(visualize_temporal_update(YrA, C_temporal, S_temporal, g_temporal, sig_temporal, A_temporal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing CNMF cannot do is merge together units that belong to the same cell. Even though we tried something similar during [initialization](#initialization), we might miss some, and it is better to do it here again. Recall the parameters:\n",
    "\n",
    "```python\n",
    "param_first_merge = {\n",
    "    'thres_corr': 0.9}\n",
    "```\n",
    "\n",
    "The idea is straight-forward and based purely on pearson correlation of temporal activities. Any units whose spatial footprints share at least one pixel are considered potential targets for merging, and any of these units that have a pearson correlation of temporal activities higher than `thres_corr` will be merged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "A_mrg, sig_mrg = unit_merge(A_temporal, sig_temporal, **param_first_merge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can visualize the results of unit merging. The left panel shows the original temporal signal, while the right panel shows the temporal signal after merging.\n",
    "<div class=\"alert alert-info\">\n",
    "Ideally, you want to see units in the left panel with <strong>too</strong> similar of signals, merged in the right penal. Adjust the <strong>thres_corr</strong> in <strong>param_first_merge</strong> accordingly.\n",
    "</div>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "opts_im = dict(plot=dict(height=500, width=1000, colorbar=True), style=dict(cmap='Viridis'))\n",
    "ranges = dict(c=(0, 4), s=(0, 0.04))\n",
    "(regrid(hv.Image(sig_temporal.rename('c'), kdims=['frame', 'unit_id'])).relabel(\"Temporal Signals Before Merge\").opts(**opts_im).redim.range(**ranges) +\n",
    "regrid(hv.Image(sig_mrg.rename('c'), kdims=['frame', 'unit_id'])).relabel(\"Temporal Signals After Merge\").opts(**opts_im).redim.range(**ranges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test parameters for second spatial update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is almost identical to the [first time](#test-parameters-for-first-spatial-update) we explore spatial parameters, except for changes in variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    A_mrg, sig_mrg = (A_mrg.chunk({c: chunks[c] for c in ['height', 'width', 'unit_id']}),\n",
    "                      sig_mrg.chunk({c: chunks[c] for c in ['frame', 'unit_id']}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    units = np.random.choice(A_mrg.coords['unit_id'], 10, replace=False)\n",
    "    units.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">  \n",
    "Again, you can simply <strong>add</strong> the values that you want to test or <strong>delete</strong> the values you are not interested in <strong>spar_ls</strong>. pragmatically, it's generally fine to use the same or a little smaller <strong>sprs_ls</strong> value as last time.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if interactive:\n",
    "    sprs_ls = [0.1, 0.2, 0.3]\n",
    "    A_dict = dict()\n",
    "    C_dict = dict()\n",
    "    for cur_sprs in sprs_ls:\n",
    "        cur_A, cur_b, cur_C, cur_f = update_spatial(\n",
    "            Y, A_mrg.sel(unit_id=units),\n",
    "            b_init, sig_mrg.sel(unit_id=units), f_init, sn_spatial, dl_wnd=20, sparse_penal=cur_sprs)\n",
    "        if cur_A.sizes['unit_id']:\n",
    "            A_dict[cur_sprs] = cur_A\n",
    "            C_dict[cur_sprs] = cur_C\n",
    "    hv_res = visualize_spatial_update(A_dict, C_dict, kdims=['sparse penalty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    display(hv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "Again, use the visualization results here to help choose the <strong>sparse_panel</strong> and <strong>dl_wnd</strong>, and pass them in the parameter.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## second spatial update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes the second iteration of spatial update. It is identical to [first spatial update](#first-spatial-update) except there are appending **it2**s after the variable names standing for \"iteration 2\", which is completely optional is you do not care about comparing results from different iterations. I hope it is very apparent now that if you want more iteration, just add more of this sections, as well as [the section below](#second-temporal-update)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "A_spatial_it2, b_spatial_it2, C_spatial_it2, f_spatial_it2 = update_spatial(\n",
    "    Y, A_mrg, b_spatial, sig_mrg, f_spatial, sn_spatial, **param_second_spatial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "opts = dict(plot=dict(height=A_init.sizes['height'], width=A_init.sizes['width'], colorbar=True), style=dict(cmap='Viridis'))\n",
    "(regrid(hv.Image(A_spatial.sum('unit_id').compute().rename('A'), kdims=['width', 'height'])).opts(**opts).relabel(\"Spatial Footprints First Update\")\n",
    "+ regrid(hv.Image((A_spatial.fillna(0) > 0).sum('unit_id').compute().rename('A'), kdims=['width', 'height']), aggregator='max').opts(**opts).relabel(\"Binary Spatial Footprints First Update\")\n",
    "+ regrid(hv.Image(A_spatial_it2.sum('unit_id').compute().rename('A'), kdims=['width', 'height'])).opts(**opts).relabel(\"Spatial Footprints Second Update\")\n",
    "+ regrid(hv.Image((A_spatial_it2 > 0).sum('unit_id').compute().rename('A'), kdims=['width', 'height']), aggregator='max').opts(**opts).relabel(\"Binary Spatial Footprints Second Update\")).cols(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "opts_im = dict(plot=dict(height=b_init.sizes['height'], width=b_init.sizes['width'], colorbar=True), style=dict(cmap='Viridis'))\n",
    "opts_cr = dict(plot=dict(height=b_init.sizes['height'], width=b_init.sizes['height'] * 2))\n",
    "(regrid(hv.Image(b_spatial, kdims=['width', 'height'])).opts(**opts_im).relabel('Background Spatial First Update')\n",
    " + datashade(hv.Curve(f_spatial, kdims=['frame'])).opts(**opts_cr).relabel('Background Temporal First Update')\n",
    " + regrid(hv.Image(b_spatial_it2, kdims=['width', 'height'])).opts(**opts_im).relabel('Background Spatial Second Update')\n",
    " + datashade(hv.Curve(f_spatial_it2, kdims=['frame'])).opts(**opts_cr).relabel('Background Temporal Second Update')\n",
    ").cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again, visualize the result of second spatial update, if not satisfying with this, feel free to reset **param_second_spatial** and rerun this session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test parameters for second temporal update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is almost identical to the [first time](#test-parameters-for-first-temporal-update) except for variable names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    units = np.random.choice(A_spatial_it2.coords['unit_id'], 10, replace=False)\n",
    "    units.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\" role=\"alert\">\n",
    "Similarly here, you can simply <strong>add</strong> the values that you want to test or <strong>delete</strong> the values you are not interested in <strong>p_ls</strong>, <strong>sprs_ls</strong>, <strong>add_ls</strong>, <strong>noise_ls</strong>. Generally, our aim here for second temporal update is that to refine the model and make the \"fitted spikes\", \"fitted signal\", and \"fitted calcium trace\" fit the \"raw signal\" better!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if interactive:\n",
    "    p_ls = [2]\n",
    "    sprs_ls = [1, 3, 5]\n",
    "    add_ls = [20]\n",
    "    noise_ls = [0.06]\n",
    "    YA_dict, C_dict, S_dict, g_dict, sig_dict, A_dict = [dict() for _ in range(6)]\n",
    "    for cur_p, cur_sprs, cur_add, cur_noise in itt.product(p_ls, sprs_ls, add_ls, noise_ls):\n",
    "        ks = (cur_p, cur_sprs, cur_add, cur_noise)\n",
    "        print(\"p:{}, sparse penalty:{}, additional lag:{}, noise frequency:{}\"\n",
    "              .format(cur_p, cur_sprs, cur_add, cur_noise))\n",
    "        YrA, cur_C, cur_S, cur_B, cur_C0, cur_sig, cur_g, cur_scal = update_temporal(\n",
    "            Y, A_spatial_it2.sel(unit_id=units), b_spatial, C_spatial_it2.sel(unit_id=units),\n",
    "            f_spatial, sn_spatial, sparse_penal=cur_sprs, p=cur_p, use_spatial=False, use_smooth=True,\n",
    "            add_lag = cur_add, noise_freq=cur_noise, cvx_sched=\"processes\", chk=chunks)\n",
    "        cur_A = A_spatial.sel(unit_id = cur_C.coords['unit_id'])\n",
    "        YA_dict[ks], C_dict[ks], S_dict[ks], g_dict[ks], sig_dict[ks], A_dict[ks] = (\n",
    "            YrA, cur_C, cur_S, cur_g, cur_sig, cur_A)\n",
    "    hv_res = visualize_temporal_update(\n",
    "        YA_dict, C_dict, S_dict, g_dict, sig_dict, A_dict,\n",
    "        kdims=['p', 'sparse penalty', 'additional lag', 'noise frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    display(hv_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## second temporal update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section is identical to the [first temporal update](#first-temporal-update) except for variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "YrA, C_temporal_it2, S_temporal_it2, B_temporal_it2, C0_temporal_it2, sig_temporal_it2, g_temporal_it2, scale_temporal_it2 = update_temporal(\n",
    "    Y, A_spatial_it2, b_spatial_it2, C_spatial_it2, f_spatial_it2, sn_spatial, **param_second_temporal)\n",
    "A_temporal_it2 = A_spatial_it2.sel(unit_id=C_temporal_it2.coords['unit_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "opts_im = dict(plot=dict(height=500, width=1000, colorbar=True), style=dict(cmap='Viridis'))\n",
    "ranges = dict(c=(0, 1.5), s=(0, 0.04))\n",
    "(regrid(hv.Image(C_temporal.rename('c'), kdims=['frame', 'unit_id'])).opts(**opts_im).relabel(\"Temporal Trace First Update\").redim.range(**ranges)\n",
    " + regrid(hv.Image(S_temporal.rename('s'), kdims=['frame', 'unit_id'])).opts(**opts_im).relabel(\"Spikes First Update\").redim.range(**ranges)\n",
    " + regrid(hv.Image(C_temporal_it2.rename('c').rename(unit_id='unit_id_it2'), kdims=['frame', 'unit_id_it2'])).opts(**opts_im).relabel(\"Temporal Trace Second Update\").redim.range(**ranges)\n",
    " + regrid(hv.Image(S_temporal_it2.rename('s').rename(unit_id='unit_id_it2'), kdims=['frame', 'unit_id_it2'])).opts(**opts_im).relabel(\"Spikes Second Update\").redim.range(**ranges)).cols(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we visualize all the units that are dropped during this step. Note that there will be an error message if no unit got dropped here, it's safe to ignore the error and move on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    h, w = A_spatial_it2.sizes['height'], A_spatial_it2.sizes['width']\n",
    "    im_opts = dict(plot=dict(height=h, width=w), style=dict(cmap='Viridis'))\n",
    "    cr_opts = dict(plot=dict(height=h, width=2*w))\n",
    "    bad_units = list(set(A_spatial_it2.coords['unit_id'].values) - set(A_temporal_it2.coords['unit_id'].values))\n",
    "    bad_units.sort()\n",
    "    hv_res = (datashade(hv.Dataset(YrA.sel(unit_id=bad_units).rename('raw'))\n",
    "                        .to(hv.Curve, kdims=['frame'])).opts(**cr_opts).relabel(\"Temporal Trace\")\n",
    "              + hv.Div('')\n",
    "              + regrid(hv.Dataset(A_spatial_it2.sel(unit_id=bad_units).rename('A'))\n",
    "                       .to(hv.Image, kdims=['width', 'height'])).opts(**im_opts).relabel(\"Spatial Footprint\")\n",
    "              + (regrid(hv.Image(A_temporal_it2.sum('unit_id').compute().rename('A'), kdims=['width', 'height']))\n",
    "                 .opts(**im_opts)).relabel(\"Spatial Footprints of Accepted Units\")).cols(2)\n",
    "    display(hv_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    visualize_temporal_update(YrA, C_temporal_it2, S_temporal_it2, g_temporal_it2, sig_temporal_it2, A_temporal_it2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## merge units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "A_mrg_it2, C_mrg_it2, add_list = unit_merge(A_temporal_it2, C_temporal_it2, [S_temporal_it2, C0_temporal_it2, g_temporal_it2, B_temporal_it2], **param_second_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "opts_im = dict(plot=dict(height=500, width=1000, colorbar=True), style=dict(cmap='Viridis'))\n",
    "ranges = dict(c=(0, 2), s=(0, 0.04))\n",
    "(regrid(hv.Image(C_temporal_it2.rename('c'), kdims=['frame', 'unit_id'])).relabel(\"Temporal Signals Before Merge\").opts(**opts_im).redim.range(**ranges) +\n",
    "regrid(hv.Image(C_mrg_it2.rename('c'), kdims=['frame', 'unit_id'])).relabel(\"Temporal Signals After Merge\").opts(**opts_im).redim.range(**ranges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_mrg_it2, C0_mrg_it2, g_mrg_it2, B_mrg_it2 = add_list[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally we save our results in the `minian` dataset. Note that you can save any other variables by calling `save_variable`. For example, you might want to consider using `sig_temporal`s instead of `C_temporal`s for your downstream analysis. Also you are not restricted to use the [netcdf](https://www.unidata.ucar.edu/software/netcdf/) format though it is recommended. [Explore the xarray documentation](http://xarray.pydata.org/en/stable/io.html) for all IO options, and moreover [numpy IO capabilities](https://docs.scipy.org/doc/numpy-1.15.0/reference/routines.io.html) since `xarray` is built on top of `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "minian.close()\n",
    "save_minian(A_mrg_it2.rename('A').compute(), **param_save_minian)\n",
    "save_minian(C_mrg_it2.rename('C').compute(), **param_save_minian)\n",
    "save_minian(S_mrg_it2.rename('S').compute(), **param_save_minian)\n",
    "save_minian(g_mrg_it2.rename('g').compute(), **param_save_minian)\n",
    "save_minian(C0_mrg_it2.rename('C0').compute(), **param_save_minian)\n",
    "save_minian(B_mrg_it2.rename('B').compute(), **param_save_minian)\n",
    "save_minian(b_spatial_it2.rename('b').compute(), **param_save_minian)\n",
    "save_minian(f_spatial_it2.rename('f').compute(), **param_save_minian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we load up the data we just saved for visualization purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minian = open_minian(dpath,\n",
    "                     fname=param_save_minian['fname'],\n",
    "                     backend=param_save_minian['backend'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we call `generate_videos` to get a video that can help us quickly visualize the results. Under defult setting, this video will be saved in your data folder. `generate_videos` takes in the dataset that contains all the needed variables, the full path to the output video file, and a `dict` specifying chunks for performance. The resulting video will have four parts - top left is the **Raw Video** after pre-processing and motion correction `minian['org']`; Top right is the **Processed Video** `minian['Y']` (that is, after pre-processing and motion correction); Bottom left is the **Residule**, that is **Raw Video** - **Units**. Bottom right is the **Units** from CNMF `minian['A'].dot(minian['C'], 'unit_id')`;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "generate_videos(\n",
    "    minian, os.path.join(dpath, param_save_minian['fname'] + \".mp4\"),\n",
    "    chk=dict(height=100, width=100, frame=1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a `CNMFViewer` to visualize the final result. \n",
    "<div class=\"alert alert-info\">\n",
    "<strong>Top Left panel</strong>-- spatial footprints of all cells (a sum projection).   \n",
    "    \n",
    "<strong>Top Middle panel</strong> `if UseAC` -- the dot product of A (spatial footprint) and C (temporal activities) matrix of selected neurons.   \n",
    "                              `if not UseAC` -- the spatial foortprints of selected neurons (a sum projection).\n",
    "             \n",
    "\n",
    "<strong>Top Right panel</strong>-- raw video after pre-processing and motion correction, which is the movie that's feed in as `org` to `CNMFViewer`, if nothing fed in it's `minian['org']`.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    " \n",
    "The <strong>Bottom Left Controller Panel</strong> has several useful features:\n",
    "\n",
    "<strong>Refresh</strong> -- to refresh the data when you switch group of units but it's not loading properly.\n",
    "\n",
    "\n",
    "<strong>Load Data</strong> -- to load the data into memory which will take some time by itself, but will make the later visualization faster.\n",
    "\n",
    "\n",
    "<strong>UseAC</strong> check box -- choose whether or not you want the middle panel to be the dot product of A (spatial footprint) and C (temporal activities) matrix of selected neurons. Note that this will make visualization process slower.\n",
    "\n",
    "\n",
    "<strong>Normalize</strong> -- to normalize the bottom middle trace and spike plot to each unit itself.\n",
    "\n",
    "\n",
    "<strong>ShowC</strong> -- to show calcium traces for each unit across time in the bottom middle plot.\n",
    "\n",
    "\n",
    "<strong>ShowS</strong> -- to show spikes for each unit across time in the bottom middle plot.\n",
    "\n",
    "\n",
    "<strong>Previous Group</strong> and <strong>Next Group</strong> buttons -- allow you to easily go back/forward to another group of units.\n",
    "\n",
    "\n",
    "<strong>Video Play Panel</strong> -- to let you actually play the top middle and right panel in real time.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "The <strong>Bottom Middle Panel</strong> is showing the plots of units along time axis. Each group will have 4-5 units show up in the plot. Combine the plot with the videos to check your CNMF results' quality. \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "The <strong>Bottom Right panel</strong>. is the labeling tool for you to manually kick out the \"bad\" unit by labelling them to `-1`. Also merge the units that are supposed to be merged but didn't by labelling them using the same label.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "if interactive:\n",
    "    cnmfviewer = CNMFViewer(minian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "hv.output(size=output_size)\n",
    "if interactive:\n",
    "    display(cnmfviewer.show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell is to save your manually changed labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactive:\n",
    "    save_minian(cnmfviewer.unit_labels, **param_save_minian)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "name": "pipeline.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
